#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 32], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 128], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 2, 64], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 2, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 64, 2], threadsPerWarp = [32, 1, 1], warpsPerCTA = [4, 1, 1], order = [0, 1, 2]}>
#linear = #ttg.linear<{register = [], lane = [[1], [2], [4], [8], [16]], warp = [[32], [64]], block = []}>
#loc = loc(unknown)
#loc1 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0)
#loc19 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0)
#loc35 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0)
#loc61 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0)
#loc66 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0)
#loc71 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":369:0)
#loc79 = loc("/root/triton/python/triton/language/standard.py":31:0)
#loc85 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0)
#loc97 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0)
#loc108 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0)
#loc111 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0)
#loc116 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0)
#loc123 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0)
#loc134 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0)
#loc150 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":483:0)
#loc155 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0)
#loc159 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0)
#loc194 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":490:0)
#loc201 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0)
#loc206 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0)
#loc213 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0)
#loc216 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0)
#loc243 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0)
#loc287 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0)
#loc291 = loc("/root/triton/python/triton/language/standard.py":174:0)
#loc295 = loc("/root/triton/python/triton/language/standard.py":166:0)
#loc299 = loc("/root/triton/python/triton/experimental/gluon/language/_standard.py":51:0)
#loc303 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":432:0)
#loc307 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":477:0)
#loc312 = loc("/root/triton/python/triton/language/standard.py":284:0)
#loc316 = loc("/root/triton/python/triton/language/standard.py":259:0)
#loc320 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0)
#loc323 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0)
#loc377 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)
#loc380 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0)
#loc408 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0)
#loc412 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0)
#loc429 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0)
#shared = #ttg.nvmma_shared<{swizzlingByteWidth = 64, transposed = false, elementBitWidth = 8}>
#shared1 = #ttg.swizzled_shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0]}>
#shared2 = #ttg.nvmma_shared<{swizzlingByteWidth = 64, transposed = true, elementBitWidth = 8}>
#smem = #ttg.shared_memory
#tmem = #ttng.tensor_memory_encoding<blockM = 128, blockN = 64, unpacked = true>
#tmem1 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = true>
#tmem2 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 1, unpacked = false>
#tmem3 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 32, unpacked = true>
#tmem4 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 1, unpacked = true>
#tmem5 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 128, unpacked = false>
#tmem6 = #ttng.tensor_memory_encoding<blockM = 128, blockN = 64, unpacked = false>
module attributes {ttg.maxnreg = 128 : i32, "ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "cuda:90", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @gluon_attention(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg4: i32 {tt.divisibility = 16 : i32} loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg5: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg8: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg9: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg10: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg11: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg12: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg13: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg14: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg15: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg16: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg17: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg18: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg19: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg20: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg21: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg22: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg23: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0), %arg24: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":826:0)) attributes {noinline = false} {
    %cst = arith.constant 1.44269502 : f32 loc(#loc2)
    %cst_0 = arith.constant 1.44269502 : f32 loc(#loc2)
    %0 = arith.mulf %arg0, %cst_0 : f32 loc(#loc2)
    %1:3 = tt.call @"__main__.get_desc_channel__TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD__(1,)cconstexpr_2__(2,)cconstexpr_1_"(%arg5, %arg6, %arg7, %arg8, %arg9) : (!tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) loc(#loc3)
    %2:3 = tt.call @"__main__.get_desc_channel__TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD__(1,)cconstexpr_8__(2,)cconstexpr_1_"(%arg10, %arg11, %arg12, %arg13, %arg14) : (!tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) loc(#loc4)
    %3:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_64____(1,)cconstexpr_fp32__(2,)cconstexpr_TensorMemoryLayout(block=(128, constexpr_64_), unpacked=True, cta_split_num=None)__(3,)cconstexpr_2__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) loc(#loc5)
    %4:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_64____(1,)cconstexpr_fp8e5__(2,)cconstexpr_NVMMASharedLayout(swizzle_byte_width=64, element_bitwidth=8, rank=2, transposed=False, fp4_padded=False, ctas_per_cga=_1, 1_, cta_split_num=_1, 1_, cta_order=_1, 0_)__(3,)cconstexpr_2__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) loc(#loc6)
    %5:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_128____(1,)cconstexpr_fp32__(2,)cconstexpr_TensorMemoryLayout(block=(128, constexpr_128_), unpacked=True, cta_split_num=None)__(3,)cconstexpr_1__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) loc(#loc7)
    %6:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_128____(1,)cconstexpr_fp32__(2,)cconstexpr_TensorMemoryLayout(block=(128, constexpr_128_), unpacked=True, cta_split_num=None)__(3,)cconstexpr_1__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) loc(#loc8)
    %7:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0, 0)cconstexpr_1__(1,)cconstexpr_int8__(2,)cconstexpr_MBarrierLayout(vec=1, per_phase=1, max_phase=1, order=_0_, ctas_per_cga=_1_, cta_split_num=_1_, cta_order=_0_)__(3,)cconstexpr_1__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) loc(#loc9)
    %8:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0, 0)cconstexpr_1__(1,)cconstexpr_int8__(2,)cconstexpr_MBarrierLayout(vec=1, per_phase=1, max_phase=1, order=_0_, ctas_per_cga=_1_, cta_split_num=_1_, cta_order=_0_)__(3,)cconstexpr_1__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) loc(#loc10)
    ttg.warp_specialize(%0, %arg2, %arg3, %arg4, %1#0, %1#1, %1#2, %2#0, %2#1, %2#2, %3#0, %3#1, %3#2, %4#0, %4#1, %4#2, %5#0, %5#1, %5#2, %6#0, %6#1, %6#2, %7#0, %7#1, %7#2, %8#0, %8#1, %8#2, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg1) attributes {requestedRegisters = array<i32: 192, 192, 24, 24, 24>}
    default {
      tt.call @"__main__._attn_fwd_correction____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%0, %arg2, %arg3, %arg4, %1#0, %1#1, %1#2, %2#0, %2#1, %2#2, %3#0, %3#1, %3#2, %4#0, %4#1, %4#2, %5#0, %5#1, %5#2, %6#0, %6#1, %6#2, %7#0, %7#1, %7#2, %8#0, %8#1, %8#2, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %arg16, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg1) : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
      ttg.warp_yield loc(#loc)
    }
    partition0(%arg25: f32 loc(unknown), %arg26: i32 loc(unknown), %arg27: i32 loc(unknown), %arg28: i32 loc(unknown), %arg29: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg30: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg31: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg32: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg33: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg34: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg35: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(unknown), %arg36: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg37: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg38: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg39: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg40: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg41: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg44: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg45: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg46: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg47: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg48: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg49: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg50: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg51: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg53: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg54: i32 loc(unknown), %arg55: i32 loc(unknown), %arg56: i64 loc(unknown), %arg57: i64 loc(unknown), %arg58: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg59: i32 loc(unknown), %arg60: i32 loc(unknown), %arg61: i64 loc(unknown), %arg62: i64 loc(unknown), %arg63: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg64: i32 loc(unknown), %arg65: i32 loc(unknown), %arg66: i64 loc(unknown), %arg67: i64 loc(unknown), %arg68: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg69: i32 loc(unknown), %arg70: i32 loc(unknown), %arg71: i64 loc(unknown), %arg72: i64 loc(unknown), %arg73: !tt.ptr<f32> loc(unknown)) num_warps(4) {
      tt.call @"__main__._attn_fwd_softmax0____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73) : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
      ttg.warp_return loc(#loc)
    }
    partition1(%arg25: f32 loc(unknown), %arg26: i32 loc(unknown), %arg27: i32 loc(unknown), %arg28: i32 loc(unknown), %arg29: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg30: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg31: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg32: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg33: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg34: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg35: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(unknown), %arg36: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg37: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg38: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg39: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg40: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg41: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg44: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg45: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg46: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg47: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg48: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg49: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg50: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg51: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg53: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg54: i32 loc(unknown), %arg55: i32 loc(unknown), %arg56: i64 loc(unknown), %arg57: i64 loc(unknown), %arg58: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg59: i32 loc(unknown), %arg60: i32 loc(unknown), %arg61: i64 loc(unknown), %arg62: i64 loc(unknown), %arg63: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg64: i32 loc(unknown), %arg65: i32 loc(unknown), %arg66: i64 loc(unknown), %arg67: i64 loc(unknown), %arg68: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg69: i32 loc(unknown), %arg70: i32 loc(unknown), %arg71: i64 loc(unknown), %arg72: i64 loc(unknown), %arg73: !tt.ptr<f32> loc(unknown)) num_warps(4) {
      tt.call @"__main__._attn_fwd_softmax1____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73) : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
      ttg.warp_return loc(#loc)
    }
    partition2(%arg25: f32 loc(unknown), %arg26: i32 loc(unknown), %arg27: i32 loc(unknown), %arg28: i32 loc(unknown), %arg29: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg30: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg31: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg32: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg33: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg34: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg35: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(unknown), %arg36: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg37: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg38: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg39: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg40: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg41: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg44: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg45: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg46: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg47: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg48: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg49: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg50: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg51: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg53: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg54: i32 loc(unknown), %arg55: i32 loc(unknown), %arg56: i64 loc(unknown), %arg57: i64 loc(unknown), %arg58: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg59: i32 loc(unknown), %arg60: i32 loc(unknown), %arg61: i64 loc(unknown), %arg62: i64 loc(unknown), %arg63: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg64: i32 loc(unknown), %arg65: i32 loc(unknown), %arg66: i64 loc(unknown), %arg67: i64 loc(unknown), %arg68: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg69: i32 loc(unknown), %arg70: i32 loc(unknown), %arg71: i64 loc(unknown), %arg72: i64 loc(unknown), %arg73: !tt.ptr<f32> loc(unknown)) num_warps(1) {
      tt.call @"__main__._attn_fwd_mma____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73) : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
      ttg.warp_return loc(#loc)
    }
    partition3(%arg25: f32 loc(unknown), %arg26: i32 loc(unknown), %arg27: i32 loc(unknown), %arg28: i32 loc(unknown), %arg29: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg30: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg31: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg32: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg33: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg34: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg35: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(unknown), %arg36: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg37: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg38: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg39: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg40: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg41: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg44: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg45: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg46: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg47: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg48: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg49: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg50: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg51: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg53: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg54: i32 loc(unknown), %arg55: i32 loc(unknown), %arg56: i64 loc(unknown), %arg57: i64 loc(unknown), %arg58: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg59: i32 loc(unknown), %arg60: i32 loc(unknown), %arg61: i64 loc(unknown), %arg62: i64 loc(unknown), %arg63: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg64: i32 loc(unknown), %arg65: i32 loc(unknown), %arg66: i64 loc(unknown), %arg67: i64 loc(unknown), %arg68: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg69: i32 loc(unknown), %arg70: i32 loc(unknown), %arg71: i64 loc(unknown), %arg72: i64 loc(unknown), %arg73: !tt.ptr<f32> loc(unknown)) num_warps(1) {
      tt.call @"__main__._attn_fwd_load____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73) : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
      ttg.warp_return loc(#loc)
    }
    partition4(%arg25: f32 loc(unknown), %arg26: i32 loc(unknown), %arg27: i32 loc(unknown), %arg28: i32 loc(unknown), %arg29: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg30: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg31: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg32: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg33: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg34: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(unknown), %arg35: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(unknown), %arg36: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg37: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg38: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(unknown), %arg39: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg40: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(unknown), %arg41: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg42: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg43: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg44: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(unknown), %arg45: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg46: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg47: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg48: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg49: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg50: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(unknown), %arg51: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg52: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(unknown), %arg53: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg54: i32 loc(unknown), %arg55: i32 loc(unknown), %arg56: i64 loc(unknown), %arg57: i64 loc(unknown), %arg58: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg59: i32 loc(unknown), %arg60: i32 loc(unknown), %arg61: i64 loc(unknown), %arg62: i64 loc(unknown), %arg63: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg64: i32 loc(unknown), %arg65: i32 loc(unknown), %arg66: i64 loc(unknown), %arg67: i64 loc(unknown), %arg68: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc(unknown), %arg69: i32 loc(unknown), %arg70: i32 loc(unknown), %arg71: i64 loc(unknown), %arg72: i64 loc(unknown), %arg73: !tt.ptr<f32> loc(unknown)) num_warps(1) {
      tt.call @"__main__._attn_fwd_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg25, %arg26, %arg27, %arg28, %arg29, %arg30, %arg31, %arg32, %arg33, %arg34, %arg35, %arg36, %arg37, %arg38, %arg39, %arg40, %arg41, %arg42, %arg43, %arg44, %arg45, %arg46, %arg47, %arg48, %arg49, %arg50, %arg51, %arg52, %arg53, %arg54, %arg55, %arg56, %arg57, %arg58, %arg59, %arg60, %arg61, %arg62, %arg63, %arg64, %arg65, %arg66, %arg67, %arg68, %arg69, %arg70, %arg71, %arg72, %arg73) : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
      ttg.warp_return loc(#loc)
    } : (f32, i32, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !tt.ptr<f32>) -> () loc(#loc)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%1#0, %1#1, %1#2) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> () loc(#loc11)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>__"(%2#0, %2#1, %2#2) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) -> () loc(#loc12)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%3#0, %3#1, %3#2) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> () loc(#loc13)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%4#0, %4#1, %4#2) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> () loc(#loc14)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%5#0, %5#1, %5#2) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc15)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%6#0, %6#1, %6#2) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc16)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%7#0, %7#1, %7#2) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc17)
    tt.call @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%8#0, %8#1, %8#2) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc18)
    tt.return loc(#loc18)
  } loc(#loc1)
  tt.func private @"__main__.get_desc_channel__TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD__(1,)cconstexpr_2__(2,)cconstexpr_1_"(%arg0: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg3: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg4: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0)) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %0:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr___sq_128_sq_, _sq_64_sq____(1,)cconstexpr_fp8e5__(2,)cconstexpr_NVMMASharedLayout(swizzle_byte_width=64, element_bitwidth=8, rank=2, transposed=False, fp4_padded=False, ctas_per_cga=_1, 1_, cta_split_num=_1, 1_, cta_order=_1, 0_)__(3,)cconstexpr_2__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) loc(#loc20)
    tt.return %0#0, %0#1, %0#2 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc21)
  ^bb1:  // no predecessors
    %1 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc22)
    %2 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc22)
    %3 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc22)
    tt.return %1, %2, %3 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc22)
  } loc(#loc19)
  tt.func private @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr___sq_128_sq_, _sq_64_sq____(1,)cconstexpr_fp8e5__(2,)cconstexpr_NVMMASharedLayout(swizzle_byte_width=64, element_bitwidth=8, rank=2, transposed=False, fp4_padded=False, ctas_per_cga=_1, 1_, cta_split_num=_1, 1_, cta_order=_1, 0_)__(3,)cconstexpr_2__(4,)cconstexpr_1_"() -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %0 = ttg.local_alloc : () -> !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc24)
    %1 = ttg.local_alloc : () -> !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc25)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc26)
    %c0_i32 = arith.constant 0 : i32 loc(#loc27)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc27)
    %3 = ttg.memdesc_subview %1[%c0_i32_0, %c0_i32] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc27)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc28)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc29)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc29)
    %4 = ttg.memdesc_subview %2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc29)
    ttng.init_barrier %4, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc30)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc31)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc31)
    %5 = ttg.memdesc_subview %2[%c0_i32_4, %c0_i32_3] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc31)
    %true = arith.constant true loc(#loc32)
    ttng.arrive_barrier %5, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc32)
    %c0_i32_5 = arith.constant 0 : i32 loc(#loc27)
    %c1_i32 = arith.constant 1 : i32 loc(#loc27)
    %6 = ttg.memdesc_subview %1[%c1_i32, %c0_i32_5] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc27)
    ttng.init_barrier %6, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc28)
    %c0_i32_6 = arith.constant 0 : i32 loc(#loc29)
    %c1_i32_7 = arith.constant 1 : i32 loc(#loc29)
    %7 = ttg.memdesc_subview %2[%c1_i32_7, %c0_i32_6] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc29)
    ttng.init_barrier %7, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc30)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc31)
    %c1_i32_9 = arith.constant 1 : i32 loc(#loc31)
    %8 = ttg.memdesc_subview %2[%c1_i32_9, %c0_i32_8] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc31)
    %true_10 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %8, 1, %true_10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc32)
    tt.return %0, %1, %2 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc33)
  ^bb1:  // no predecessors
    %9 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc34)
    %10 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
    %11 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
    tt.return %9, %10, %11 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
  } loc(#loc23)
  tt.func private @"__main__.get_desc_channel__TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD__(1,)cconstexpr_8__(2,)cconstexpr_1_"(%arg0: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg3: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0), %arg4: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":230:0)) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %0:3 = tt.call @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr___sq_128_sq_, _sq_64_sq____(1,)cconstexpr_fp8e5__(2,)cconstexpr_NVMMASharedLayout(swizzle_byte_width=64, element_bitwidth=8, rank=2, transposed=False, fp4_padded=False, ctas_per_cga=_1, 1_, cta_split_num=_1, 1_, cta_order=_1, 0_)__(3,)cconstexpr_8__(4,)cconstexpr_1_"() : () -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) loc(#loc20)
    tt.return %0#0, %0#1, %0#2 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc21)
  ^bb1:  // no predecessors
    %1 = ub.poison : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc22)
    %2 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc22)
    %3 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc22)
    tt.return %1, %2, %3 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc22)
  } loc(#loc19)
  tt.func private @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr___sq_128_sq_, _sq_64_sq____(1,)cconstexpr_fp8e5__(2,)cconstexpr_NVMMASharedLayout(swizzle_byte_width=64, element_bitwidth=8, rank=2, transposed=False, fp4_padded=False, ctas_per_cga=_1, 1_, cta_split_num=_1, 1_, cta_order=_1, 0_)__(3,)cconstexpr_8__(4,)cconstexpr_1_"() -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %0 = ttg.local_alloc : () -> !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc24)
    %1 = ttg.local_alloc : () -> !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc25)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc26)
    %c0_i32 = arith.constant 0 : i32 loc(#loc27)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc27)
    %3 = ttg.memdesc_subview %1[%c0_i32_0, %c0_i32] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc29)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc29)
    %4 = ttg.memdesc_subview %2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %4, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc31)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc31)
    %5 = ttg.memdesc_subview %2[%c0_i32_4, %c0_i32_3] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true = arith.constant true loc(#loc32)
    ttng.arrive_barrier %5, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_5 = arith.constant 0 : i32 loc(#loc27)
    %c1_i32 = arith.constant 1 : i32 loc(#loc27)
    %6 = ttg.memdesc_subview %1[%c1_i32, %c0_i32_5] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %6, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_6 = arith.constant 0 : i32 loc(#loc29)
    %c1_i32_7 = arith.constant 1 : i32 loc(#loc29)
    %7 = ttg.memdesc_subview %2[%c1_i32_7, %c0_i32_6] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %7, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc31)
    %c1_i32_9 = arith.constant 1 : i32 loc(#loc31)
    %8 = ttg.memdesc_subview %2[%c1_i32_9, %c0_i32_8] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_10 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %8, 1, %true_10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_11 = arith.constant 0 : i32 loc(#loc27)
    %c2_i32 = arith.constant 2 : i32 loc(#loc27)
    %9 = ttg.memdesc_subview %1[%c2_i32, %c0_i32_11] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %9, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_12 = arith.constant 0 : i32 loc(#loc29)
    %c2_i32_13 = arith.constant 2 : i32 loc(#loc29)
    %10 = ttg.memdesc_subview %2[%c2_i32_13, %c0_i32_12] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %10, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_14 = arith.constant 0 : i32 loc(#loc31)
    %c2_i32_15 = arith.constant 2 : i32 loc(#loc31)
    %11 = ttg.memdesc_subview %2[%c2_i32_15, %c0_i32_14] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_16 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %11, 1, %true_16 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_17 = arith.constant 0 : i32 loc(#loc27)
    %c3_i32 = arith.constant 3 : i32 loc(#loc27)
    %12 = ttg.memdesc_subview %1[%c3_i32, %c0_i32_17] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %12, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_18 = arith.constant 0 : i32 loc(#loc29)
    %c3_i32_19 = arith.constant 3 : i32 loc(#loc29)
    %13 = ttg.memdesc_subview %2[%c3_i32_19, %c0_i32_18] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %13, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_20 = arith.constant 0 : i32 loc(#loc31)
    %c3_i32_21 = arith.constant 3 : i32 loc(#loc31)
    %14 = ttg.memdesc_subview %2[%c3_i32_21, %c0_i32_20] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_22 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %14, 1, %true_22 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_23 = arith.constant 0 : i32 loc(#loc27)
    %c4_i32 = arith.constant 4 : i32 loc(#loc27)
    %15 = ttg.memdesc_subview %1[%c4_i32, %c0_i32_23] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %15, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_24 = arith.constant 0 : i32 loc(#loc29)
    %c4_i32_25 = arith.constant 4 : i32 loc(#loc29)
    %16 = ttg.memdesc_subview %2[%c4_i32_25, %c0_i32_24] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %16, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_26 = arith.constant 0 : i32 loc(#loc31)
    %c4_i32_27 = arith.constant 4 : i32 loc(#loc31)
    %17 = ttg.memdesc_subview %2[%c4_i32_27, %c0_i32_26] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_28 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %17, 1, %true_28 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_29 = arith.constant 0 : i32 loc(#loc27)
    %c5_i32 = arith.constant 5 : i32 loc(#loc27)
    %18 = ttg.memdesc_subview %1[%c5_i32, %c0_i32_29] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %18, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_30 = arith.constant 0 : i32 loc(#loc29)
    %c5_i32_31 = arith.constant 5 : i32 loc(#loc29)
    %19 = ttg.memdesc_subview %2[%c5_i32_31, %c0_i32_30] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %19, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_32 = arith.constant 0 : i32 loc(#loc31)
    %c5_i32_33 = arith.constant 5 : i32 loc(#loc31)
    %20 = ttg.memdesc_subview %2[%c5_i32_33, %c0_i32_32] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_34 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %20, 1, %true_34 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_35 = arith.constant 0 : i32 loc(#loc27)
    %c6_i32 = arith.constant 6 : i32 loc(#loc27)
    %21 = ttg.memdesc_subview %1[%c6_i32, %c0_i32_35] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %21, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_36 = arith.constant 0 : i32 loc(#loc29)
    %c6_i32_37 = arith.constant 6 : i32 loc(#loc29)
    %22 = ttg.memdesc_subview %2[%c6_i32_37, %c0_i32_36] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %22, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_38 = arith.constant 0 : i32 loc(#loc31)
    %c6_i32_39 = arith.constant 6 : i32 loc(#loc31)
    %23 = ttg.memdesc_subview %2[%c6_i32_39, %c0_i32_38] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_40 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %23, 1, %true_40 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    %c0_i32_41 = arith.constant 0 : i32 loc(#loc27)
    %c7_i32 = arith.constant 7 : i32 loc(#loc27)
    %24 = ttg.memdesc_subview %1[%c7_i32, %c0_i32_41] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc27)
    ttng.init_barrier %24, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc28)
    %c0_i32_42 = arith.constant 0 : i32 loc(#loc29)
    %c7_i32_43 = arith.constant 7 : i32 loc(#loc29)
    %25 = ttg.memdesc_subview %2[%c7_i32_43, %c0_i32_42] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc29)
    ttng.init_barrier %25, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc30)
    %c0_i32_44 = arith.constant 0 : i32 loc(#loc31)
    %c7_i32_45 = arith.constant 7 : i32 loc(#loc31)
    %26 = ttg.memdesc_subview %2[%c7_i32_45, %c0_i32_44] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc31)
    %true_46 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %26, 1, %true_46 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc32)
    tt.return %0, %1, %2 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc33)
  ^bb1:  // no predecessors
    %27 = ub.poison : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc34)
    %28 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc34)
    %29 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc34)
    tt.return %27, %28, %29 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc34)
  } loc(#loc23)
  tt.func private @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_64____(1,)cconstexpr_fp32__(2,)cconstexpr_TensorMemoryLayout(block=(128, constexpr_64_), unpacked=True, cta_split_num=None)__(3,)cconstexpr_2__(4,)cconstexpr_1_"() -> (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %result = ttng.tmem_alloc : () -> !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc24)
    %0 = ttg.local_alloc : () -> !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc25)
    %1 = ttg.local_alloc : () -> !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc26)
    %c0_i32 = arith.constant 0 : i32 loc(#loc27)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc27)
    %2 = ttg.memdesc_subview %0[%c0_i32_0, %c0_i32] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc27)
    ttng.init_barrier %2, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc28)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc29)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc29)
    %3 = ttg.memdesc_subview %1[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc29)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc30)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc31)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc31)
    %4 = ttg.memdesc_subview %1[%c0_i32_4, %c0_i32_3] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc31)
    %true = arith.constant true loc(#loc32)
    ttng.arrive_barrier %4, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc32)
    %c0_i32_5 = arith.constant 0 : i32 loc(#loc27)
    %c1_i32 = arith.constant 1 : i32 loc(#loc27)
    %5 = ttg.memdesc_subview %0[%c1_i32, %c0_i32_5] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc27)
    ttng.init_barrier %5, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc28)
    %c0_i32_6 = arith.constant 0 : i32 loc(#loc29)
    %c1_i32_7 = arith.constant 1 : i32 loc(#loc29)
    %6 = ttg.memdesc_subview %1[%c1_i32_7, %c0_i32_6] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc29)
    ttng.init_barrier %6, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc30)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc31)
    %c1_i32_9 = arith.constant 1 : i32 loc(#loc31)
    %7 = ttg.memdesc_subview %1[%c1_i32_9, %c0_i32_8] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc31)
    %true_10 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %7, 1, %true_10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc32)
    tt.return %result, %0, %1 : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc33)
  ^bb1:  // no predecessors
    %8 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc34)
    %9 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
    %10 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
    tt.return %8, %9, %10 : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
  } loc(#loc23)
  tt.func private @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_64____(1,)cconstexpr_fp8e5__(2,)cconstexpr_NVMMASharedLayout(swizzle_byte_width=64, element_bitwidth=8, rank=2, transposed=False, fp4_padded=False, ctas_per_cga=_1, 1_, cta_split_num=_1, 1_, cta_order=_1, 0_)__(3,)cconstexpr_2__(4,)cconstexpr_1_"() -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %0 = ttg.local_alloc : () -> !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc24)
    %1 = ttg.local_alloc : () -> !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc25)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc26)
    %c0_i32 = arith.constant 0 : i32 loc(#loc27)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc27)
    %3 = ttg.memdesc_subview %1[%c0_i32_0, %c0_i32] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc27)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc28)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc29)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc29)
    %4 = ttg.memdesc_subview %2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc29)
    ttng.init_barrier %4, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc30)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc31)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc31)
    %5 = ttg.memdesc_subview %2[%c0_i32_4, %c0_i32_3] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc31)
    %true = arith.constant true loc(#loc32)
    ttng.arrive_barrier %5, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc32)
    %c0_i32_5 = arith.constant 0 : i32 loc(#loc27)
    %c1_i32 = arith.constant 1 : i32 loc(#loc27)
    %6 = ttg.memdesc_subview %1[%c1_i32, %c0_i32_5] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc27)
    ttng.init_barrier %6, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc28)
    %c0_i32_6 = arith.constant 0 : i32 loc(#loc29)
    %c1_i32_7 = arith.constant 1 : i32 loc(#loc29)
    %7 = ttg.memdesc_subview %2[%c1_i32_7, %c0_i32_6] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc29)
    ttng.init_barrier %7, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc30)
    %c0_i32_8 = arith.constant 0 : i32 loc(#loc31)
    %c1_i32_9 = arith.constant 1 : i32 loc(#loc31)
    %8 = ttg.memdesc_subview %2[%c1_i32_9, %c0_i32_8] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc31)
    %true_10 = arith.constant true loc(#loc32)
    ttng.arrive_barrier %8, 1, %true_10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc32)
    tt.return %0, %1, %2 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc33)
  ^bb1:  // no predecessors
    %9 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc34)
    %10 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
    %11 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
    tt.return %9, %10, %11 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc34)
  } loc(#loc23)
  tt.func private @"__main__.Channel.<locals>.ChannelType.alloc____(0,)cconstexpr__constexpr_128_, constexpr_128____(1,)cconstexpr_fp32__(2,)cconstexpr_TensorMemoryLayout(block=(128, constexpr_128_), unpacked=True, cta_split_num=None)__(3,)cconstexpr_1__(4,)cconstexpr_1_"() -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %result = ttng.tmem_alloc : () -> !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc24)
    %0 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc25)
    %1 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc26)
    %c0_i32 = arith.constant 0 : i32 loc(#loc27)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc27)
    %2 = ttg.memdesc_subview %0[%c0_i32_0, %c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc27)
    ttng.init_barrier %2, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc28)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc29)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc29)
    %3 = ttg.memdesc_subview %1[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc29)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc30)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc31)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc31)
    %4 = ttg.memdesc_subview %1[%c0_i32_4, %c0_i32_3] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc31)
    %true = arith.constant true loc(#loc32)
    ttng.arrive_barrier %4, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc32)
    tt.return %result, %0, %1 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc33)
  ^bb1:  // no predecessors
    %5 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc34)
    %6 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc34)
    %7 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc34)
    tt.return %5, %6, %7 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc34)
  } loc(#loc23)
  tt.func private @"__main__.Channel.<locals>.ChannelType.alloc____(0, 0)cconstexpr_1__(1,)cconstexpr_int8__(2,)cconstexpr_MBarrierLayout(vec=1, per_phase=1, max_phase=1, order=_0_, ctas_per_cga=_1_, cta_split_num=_1_, cta_order=_0_)__(3,)cconstexpr_1__(4,)cconstexpr_1_"() -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) attributes {noinline = false} {
    %0 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc24)
    %1 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc25)
    %2 = ttg.local_alloc : () -> !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc26)
    %c0_i32 = arith.constant 0 : i32 loc(#loc27)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc27)
    %3 = ttg.memdesc_subview %1[%c0_i32_0, %c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc27)
    ttng.init_barrier %3, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc28)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc29)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc29)
    %4 = ttg.memdesc_subview %2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc29)
    ttng.init_barrier %4, 1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc30)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc31)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc31)
    %5 = ttg.memdesc_subview %2[%c0_i32_4, %c0_i32_3] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc31)
    %true = arith.constant true loc(#loc32)
    ttng.arrive_barrier %5, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc32)
    tt.return %0, %1, %2 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc33)
  ^bb1:  // no predecessors
    %6 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc34)
    %7 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc34)
    %8 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc34)
    tt.return %6, %7, %8 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc34)
  } loc(#loc23)
  tt.func private @"__main__._attn_fwd_correction____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg4: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg5: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg6: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg7: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg8: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg9: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg13: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg14: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg15: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg16: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg17: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg19: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg20: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg21: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg22: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg23: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg24: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg25: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg28: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg29: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg30: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg31: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg32: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg33: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg34: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg35: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg36: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg37: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg38: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg39: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg40: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg41: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg42: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg43: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg44: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg45: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg46: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg47: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0), %arg48: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":785:0)) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc36)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc36)
    %0 = ttg.memdesc_subview %arg16[%c0_i32, %c0_i32_0, %c0_i32_0] : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc36)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc37)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc37)
    %1 = ttg.memdesc_subview %arg19[%c0_i32_1, %c0_i32_2, %c0_i32_2] : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc37)
    %2:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg22, %arg23, %arg24) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc38)
    %3:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg25, %arg26, %arg27) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc39)
    %4:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg10, %arg11, %arg12) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc40)
    %5:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg13, %arg14, %arg15) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc41)
    %6:8 = tt.call @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0, %arg1, %arg2, %arg3) : (f32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc42)
    %c78_i32 = arith.constant 78 : i32 loc(#loc43)
    %7 = arith.bitcast %6#4 : i32 to i32 loc(#loc43)
    %8 = arith.bitcast %6#7 : i32 to i32 loc(#loc43)
    %9 = arith.bitcast %c78_i32 : i32 to i32 loc(#loc43)
    %10 = ub.poison : i32 loc(#loc43)
    %11:20 = scf.for %arg49 = %7 to %8 step %9 iter_args(%arg50 = %2#0, %arg51 = %2#1, %arg52 = %2#2, %arg53 = %2#3, %arg54 = %2#4, %arg55 = %3#0, %arg56 = %3#1, %arg57 = %3#2, %arg58 = %3#3, %arg59 = %3#4, %arg60 = %4#0, %arg61 = %4#1, %arg62 = %4#2, %arg63 = %4#3, %arg64 = %4#4, %arg65 = %5#0, %arg66 = %5#1, %arg67 = %5#2, %arg68 = %5#3, %arg69 = %5#4) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32)  : i32 {
      %12:8 = tt.call @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%6#0, %6#1, %6#2, %6#3, %6#4, %6#5, %6#6, %6#7, %arg49) : (f32, i32, i32, i32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc44)
      %13:2 = tt.call @"__main__.AttentionProgram.get_fused_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_1_"(%12#0, %12#1, %12#2, %12#3, %12#4, %12#5, %12#6, %12#7) : (f32, i32, i32, i32, i32, i32, i32, i32) -> (i32, i32) loc(#loc45)
      %14 = arith.extsi %13#1 : i32 to i64 loc(#loc46)
      %15 = arith.extsi %13#0 : i32 to i64 loc(#loc46)
      %16 = arith.subi %14, %15 : i64 loc(#loc46)
      %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc46)
      %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc46)
      %17 = arith.cmpi sle, %16, %c2147483647_i64 : i64 loc(#loc46)
      %18 = arith.cmpi sge, %16, %c-2147483648_i64 : i64 loc(#loc46)
      %19 = arith.andi %17, %18 : i1 loc(#loc46)
      %20 = arith.subi %13#1, %13#0 : i32 loc(#loc46)
      %c128_i32 = arith.constant 128 : i32 loc(#loc47)
      %c128_i32_3 = arith.constant 128 : i32 loc(#loc47)
      %21 = arith.divsi %20, %c128_i32_3 : i32 loc(#loc47)
      %22:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg50, %arg51, %arg52, %arg53, %arg54) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc48)
      %true = arith.constant true loc(#loc49)
      ttng.arrive_barrier %22#1, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc49)
      %23:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg55, %arg56, %arg57, %arg58, %arg59) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc50)
      %true_4 = arith.constant true loc(#loc51)
      ttng.arrive_barrier %23#1, 1, %true_4 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc51)
      %c1_i32 = arith.constant 1 : i32 loc(#loc52)
      %c1_i32_5 = arith.constant 1 : i32 loc(#loc52)
      %24 = arith.extsi %21 : i32 to i64 loc(#loc52)
      %25 = arith.extsi %c1_i32_5 : i32 to i64 loc(#loc52)
      %26 = arith.subi %24, %25 : i64 loc(#loc52)
      %c2147483647_i64_6 = arith.constant 2147483647 : i64 loc(#loc52)
      %c-2147483648_i64_7 = arith.constant -2147483648 : i64 loc(#loc52)
      %27 = arith.cmpi sle, %26, %c2147483647_i64_6 : i64 loc(#loc52)
      %28 = arith.cmpi sge, %26, %c-2147483648_i64_7 : i64 loc(#loc52)
      %29 = arith.andi %27, %28 : i1 loc(#loc52)
      %30 = arith.subi %21, %c1_i32_5 : i32 loc(#loc52)
      %c1_i32_8 = arith.constant 1 : i32 loc(#loc52)
      %c1_i32_9 = arith.constant 1 : i32 loc(#loc52)
      %31 = arith.extsi %21 : i32 to i64 loc(#loc52)
      %32 = arith.extsi %c1_i32_9 : i32 to i64 loc(#loc52)
      %33 = arith.subi %31, %32 : i64 loc(#loc52)
      %c2147483647_i64_10 = arith.constant 2147483647 : i64 loc(#loc52)
      %c-2147483648_i64_11 = arith.constant -2147483648 : i64 loc(#loc52)
      %34 = arith.cmpi sle, %33, %c2147483647_i64_10 : i64 loc(#loc52)
      %35 = arith.cmpi sge, %33, %c-2147483648_i64_11 : i64 loc(#loc52)
      %36 = arith.andi %34, %35 : i1 loc(#loc52)
      %37 = arith.subi %21, %c1_i32_9 : i32 loc(#loc52)
      %c0_i32_12 = arith.constant 0 : i32 loc(#loc53)
      %c1_i32_13 = arith.constant 1 : i32 loc(#loc53)
      %38 = arith.bitcast %c0_i32_12 : i32 to i32 loc(#loc53)
      %39 = arith.bitcast %37 : i32 to i32 loc(#loc53)
      %40 = arith.bitcast %c1_i32_13 : i32 to i32 loc(#loc53)
      %41 = ub.poison : i32 loc(#loc53)
      %42:15 = scf.for %arg70 = %38 to %39 step %40 iter_args(%arg71 = %22#2, %arg72 = %22#3, %arg73 = %22#4, %arg74 = %22#5, %arg75 = %22#6, %arg76 = %23#2, %arg77 = %23#3, %arg78 = %23#4, %arg79 = %23#5, %arg80 = %23#6, %arg81 = %arg60, %arg82 = %arg61, %arg83 = %arg62, %arg84 = %arg63, %arg85 = %arg64) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32)  : i32 {
        %45:10 = tt.call @"__main__._attn_fwd_correction_rescale____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0, %arg1, %arg2, %arg3, %0, %arg71, %arg72, %arg73, %arg74, %arg75, %arg81, %arg82, %arg83, %arg84, %arg85) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc54)
        %46:10 = tt.call @"__main__._attn_fwd_correction_rescale____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0, %arg1, %arg2, %arg3, %1, %arg76, %arg77, %arg78, %arg79, %arg80, %45#5, %45#6, %45#7, %45#8, %45#9) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc55)
        scf.yield %45#0, %45#1, %45#2, %45#3, %45#4, %46#0, %46#1, %46#2, %46#3, %46#4, %46#5, %46#6, %46#7, %46#8, %46#9 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc56)
      } loc(#loc53)
      %43:15 = tt.call @"__main__._attn_fwd_correction_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_Pfp32___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0, %arg1, %arg2, %arg3, %12#0, %12#1, %12#2, %12#3, %12#4, %12#5, %12#6, %12#7, %0, %arg48, %42#0, %42#1, %42#2, %42#3, %42#4, %arg65, %arg66, %arg67, %arg68, %arg69, %42#10, %42#11, %42#12, %42#13, %42#14) : (f32, i32, i32, i32, f32, i32, i32, i32, i32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !tt.ptr<f32>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc57)
      %44:15 = tt.call @"__main__._attn_fwd_correction_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_Pfp32___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0, %arg1, %arg2, %arg3, %12#0, %12#1, %12#2, %12#3, %12#4, %12#5, %12#6, %12#7, %1, %arg48, %42#5, %42#6, %42#7, %42#8, %42#9, %43#5, %43#6, %43#7, %43#8, %43#9, %43#10, %43#11, %43#12, %43#13, %43#14) : (f32, i32, i32, i32, f32, i32, i32, i32, i32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !tt.ptr<f32>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc58)
      scf.yield %43#0, %43#1, %43#2, %43#3, %43#4, %44#0, %44#1, %44#2, %44#3, %44#4, %44#10, %44#11, %44#12, %44#13, %44#14, %44#5, %44#6, %44#7, %44#8, %44#9 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc59)
    } loc(#loc43)
    tt.return loc(#loc60)
  } loc(#loc35)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0)) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc62)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc63)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc64)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc65)
    %1 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %2 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %3 = ub.poison : i32 loc(#loc65)
    %4 = ub.poison : i32 loc(#loc65)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc65)
  } loc(#loc61)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0)) -> (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc62)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc63)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc64)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc65)
    %1 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %2 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %3 = ub.poison : i32 loc(#loc65)
    %4 = ub.poison : i32 loc(#loc65)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc65)
  } loc(#loc61)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0)) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc67)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc68)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc69)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc70)
    %1 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %2 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %3 = ub.poison : i32 loc(#loc70)
    %4 = ub.poison : i32 loc(#loc70)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc70)
  } loc(#loc66)
  tt.func private @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":369:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":369:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":369:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":369:0)) -> (f32, i32, i32, i32, i32, i32, i32, i32) attributes {noinline = false} {
    %0 = tt.get_program_id x : i32 loc(#loc72)
    %1 = tt.call @"triton.language.standard.cdiv__i32__(1,)cconstexpr_256_"(%arg3) : (i32) -> i32 loc(#loc73)
    %2 = arith.extsi %arg1 : i32 to i64 loc(#loc74)
    %3 = arith.extsi %arg2 : i32 to i64 loc(#loc74)
    %4 = arith.muli %2, %3 : i64 loc(#loc74)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc74)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc74)
    %5 = arith.cmpi sle, %4, %c2147483647_i64 : i64 loc(#loc74)
    %6 = arith.cmpi sge, %4, %c-2147483648_i64 : i64 loc(#loc74)
    %7 = arith.andi %5, %6 : i1 loc(#loc74)
    %8 = arith.muli %arg1, %arg2 : i32 loc(#loc74)
    %c8_i32 = arith.constant 8 : i32 loc(#loc75)
    %c8_i32_0 = arith.constant 8 : i32 loc(#loc75)
    %9 = arith.extsi %1 : i32 to i64 loc(#loc75)
    %10 = arith.extsi %c8_i32_0 : i32 to i64 loc(#loc75)
    %11 = arith.muli %9, %10 : i64 loc(#loc75)
    %c2147483647_i64_1 = arith.constant 2147483647 : i64 loc(#loc75)
    %c-2147483648_i64_2 = arith.constant -2147483648 : i64 loc(#loc75)
    %12 = arith.cmpi sle, %11, %c2147483647_i64_1 : i64 loc(#loc75)
    %13 = arith.cmpi sge, %11, %c-2147483648_i64_2 : i64 loc(#loc75)
    %14 = arith.andi %12, %13 : i1 loc(#loc75)
    %15 = arith.muli %1, %c8_i32_0 : i32 loc(#loc75)
    %16 = arith.extsi %1 : i32 to i64 loc(#loc76)
    %17 = arith.extsi %8 : i32 to i64 loc(#loc76)
    %18 = arith.muli %16, %17 : i64 loc(#loc76)
    %c2147483647_i64_3 = arith.constant 2147483647 : i64 loc(#loc76)
    %c-2147483648_i64_4 = arith.constant -2147483648 : i64 loc(#loc76)
    %19 = arith.cmpi sle, %18, %c2147483647_i64_3 : i64 loc(#loc76)
    %20 = arith.cmpi sge, %18, %c-2147483648_i64_4 : i64 loc(#loc76)
    %21 = arith.andi %19, %20 : i1 loc(#loc76)
    %22 = arith.muli %1, %8 : i32 loc(#loc76)
    tt.return %arg0, %arg1, %arg2, %arg3, %0, %8, %15, %22 : f32, i32, i32, i32, i32, i32, i32, i32 loc(#loc77)
  ^bb1:  // no predecessors
    %23 = ub.poison : f32 loc(#loc78)
    %24 = ub.poison : i32 loc(#loc78)
    %25 = ub.poison : i32 loc(#loc78)
    %26 = ub.poison : i32 loc(#loc78)
    %27 = ub.poison : i32 loc(#loc78)
    %28 = ub.poison : i32 loc(#loc78)
    %29 = ub.poison : i32 loc(#loc78)
    %30 = ub.poison : i32 loc(#loc78)
    tt.return %23, %24, %25, %26, %27, %28, %29, %30 : f32, i32, i32, i32, i32, i32, i32, i32 loc(#loc78)
  } loc(#loc71)
  tt.func private @"triton.language.standard.cdiv__i32__(1,)cconstexpr_256_"(%arg0: i32 loc("/root/triton/python/triton/language/standard.py":31:0)) -> i32 attributes {noinline = false} {
    %c256_i32 = arith.constant 256 : i32 loc(#loc80)
    %c256_i32_0 = arith.constant 256 : i32 loc(#loc80)
    %0 = arith.extsi %arg0 : i32 to i64 loc(#loc80)
    %1 = arith.extsi %c256_i32_0 : i32 to i64 loc(#loc80)
    %2 = arith.addi %0, %1 : i64 loc(#loc80)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc80)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc80)
    %3 = arith.cmpi sle, %2, %c2147483647_i64 : i64 loc(#loc80)
    %4 = arith.cmpi sge, %2, %c-2147483648_i64 : i64 loc(#loc80)
    %5 = arith.andi %3, %4 : i1 loc(#loc80)
    %6 = arith.addi %arg0, %c256_i32_0 : i32 loc(#loc80)
    %c1_i32 = arith.constant 1 : i32 loc(#loc81)
    %c1_i32_1 = arith.constant 1 : i32 loc(#loc81)
    %7 = arith.extsi %6 : i32 to i64 loc(#loc81)
    %8 = arith.extsi %c1_i32_1 : i32 to i64 loc(#loc81)
    %9 = arith.subi %7, %8 : i64 loc(#loc81)
    %c2147483647_i64_2 = arith.constant 2147483647 : i64 loc(#loc81)
    %c-2147483648_i64_3 = arith.constant -2147483648 : i64 loc(#loc81)
    %10 = arith.cmpi sle, %9, %c2147483647_i64_2 : i64 loc(#loc81)
    %11 = arith.cmpi sge, %9, %c-2147483648_i64_3 : i64 loc(#loc81)
    %12 = arith.andi %10, %11 : i1 loc(#loc81)
    %13 = arith.subi %6, %c1_i32_1 : i32 loc(#loc81)
    %c256_i32_4 = arith.constant 256 : i32 loc(#loc82)
    %c256_i32_5 = arith.constant 256 : i32 loc(#loc82)
    %14 = arith.divsi %13, %c256_i32_5 : i32 loc(#loc82)
    tt.return %14 : i32 loc(#loc83)
  ^bb1:  // no predecessors
    %15 = ub.poison : i32 loc(#loc84)
    tt.return %15 : i32 loc(#loc84)
  } loc(#loc79)
  tt.func private @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0), %arg8: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":378:0)) -> (f32, i32, i32, i32, i32, i32, i32, i32) attributes {noinline = false} {
    %0 = arith.divsi %arg8, %arg6 : i32 loc(#loc86)
    %c8_i32 = arith.constant 8 : i32 loc(#loc87)
    %c8_i32_0 = arith.constant 8 : i32 loc(#loc87)
    %1 = arith.extsi %0 : i32 to i64 loc(#loc87)
    %2 = arith.extsi %c8_i32_0 : i32 to i64 loc(#loc87)
    %3 = arith.muli %1, %2 : i64 loc(#loc87)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc87)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc87)
    %4 = arith.cmpi sle, %3, %c2147483647_i64 : i64 loc(#loc87)
    %5 = arith.cmpi sge, %3, %c-2147483648_i64 : i64 loc(#loc87)
    %6 = arith.andi %4, %5 : i1 loc(#loc87)
    %7 = arith.muli %0, %c8_i32_0 : i32 loc(#loc87)
    %8 = arith.extsi %arg5 : i32 to i64 loc(#loc88)
    %9 = arith.extsi %7 : i32 to i64 loc(#loc88)
    %10 = arith.subi %8, %9 : i64 loc(#loc88)
    %c2147483647_i64_1 = arith.constant 2147483647 : i64 loc(#loc88)
    %c-2147483648_i64_2 = arith.constant -2147483648 : i64 loc(#loc88)
    %11 = arith.cmpi sle, %10, %c2147483647_i64_1 : i64 loc(#loc88)
    %12 = arith.cmpi sge, %10, %c-2147483648_i64_2 : i64 loc(#loc88)
    %13 = arith.andi %11, %12 : i1 loc(#loc88)
    %14 = arith.subi %arg5, %7 : i32 loc(#loc88)
    %c8_i32_3 = arith.constant 8 : i32 loc(#loc89)
    %15 = arith.minsi %14, %c8_i32_3 : i32 loc(#loc89)
    %16 = arith.remsi %arg8, %15 : i32 loc(#loc90)
    %17 = arith.extsi %7 : i32 to i64 loc(#loc91)
    %18 = arith.extsi %16 : i32 to i64 loc(#loc91)
    %19 = arith.addi %17, %18 : i64 loc(#loc91)
    %c2147483647_i64_4 = arith.constant 2147483647 : i64 loc(#loc91)
    %c-2147483648_i64_5 = arith.constant -2147483648 : i64 loc(#loc91)
    %20 = arith.cmpi sle, %19, %c2147483647_i64_4 : i64 loc(#loc91)
    %21 = arith.cmpi sge, %19, %c-2147483648_i64_5 : i64 loc(#loc91)
    %22 = arith.andi %20, %21 : i1 loc(#loc91)
    %23 = arith.addi %7, %16 : i32 loc(#loc91)
    %24 = arith.remsi %arg8, %arg6 : i32 loc(#loc92)
    %25 = arith.divsi %24, %15 : i32 loc(#loc93)
    %26:8 = tt.call @"__main__.AttentionConfig.get_program____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_i32_i32__"(%arg0, %arg1, %arg2, %arg3, %25, %23) : (f32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc94)
    tt.return %26#0, %26#1, %26#2, %26#3, %26#4, %26#5, %26#6, %26#7 : f32, i32, i32, i32, i32, i32, i32, i32 loc(#loc95)
  ^bb1:  // no predecessors
    %27 = ub.poison : f32 loc(#loc96)
    %28 = ub.poison : i32 loc(#loc96)
    %29 = ub.poison : i32 loc(#loc96)
    %30 = ub.poison : i32 loc(#loc96)
    %31 = ub.poison : i32 loc(#loc96)
    %32 = ub.poison : i32 loc(#loc96)
    %33 = ub.poison : i32 loc(#loc96)
    %34 = ub.poison : i32 loc(#loc96)
    tt.return %27, %28, %29, %30, %31, %32, %33, %34 : f32, i32, i32, i32, i32, i32, i32, i32 loc(#loc96)
  } loc(#loc85)
  tt.func private @"__main__.AttentionConfig.get_program____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_i32_i32__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":341:0)) -> (f32, i32, i32, i32, i32, i32, i32, i32) attributes {noinline = false} {
    %0 = arith.divsi %arg5, %arg2 : i32 loc(#loc98)
    %1 = arith.remsi %arg5, %arg2 : i32 loc(#loc99)
    %2 = arith.extsi %arg3 : i32 to i64 loc(#loc100)
    %3 = arith.extsi %arg2 : i32 to i64 loc(#loc100)
    %4 = arith.muli %2, %3 : i64 loc(#loc100)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc100)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc100)
    %5 = arith.cmpi sle, %4, %c2147483647_i64 : i64 loc(#loc100)
    %6 = arith.cmpi sge, %4, %c-2147483648_i64 : i64 loc(#loc100)
    %7 = arith.andi %5, %6 : i1 loc(#loc100)
    %8 = arith.muli %arg3, %arg2 : i32 loc(#loc100)
    %9 = arith.extsi %0 : i32 to i64 loc(#loc101)
    %10 = arith.extsi %8 : i32 to i64 loc(#loc101)
    %11 = arith.muli %9, %10 : i64 loc(#loc101)
    %c2147483647_i64_0 = arith.constant 2147483647 : i64 loc(#loc101)
    %c-2147483648_i64_1 = arith.constant -2147483648 : i64 loc(#loc101)
    %12 = arith.cmpi sle, %11, %c2147483647_i64_0 : i64 loc(#loc101)
    %13 = arith.cmpi sge, %11, %c-2147483648_i64_1 : i64 loc(#loc101)
    %14 = arith.andi %12, %13 : i1 loc(#loc101)
    %15 = arith.muli %0, %8 : i32 loc(#loc101)
    %16 = arith.extsi %1 : i32 to i64 loc(#loc102)
    %17 = arith.extsi %arg3 : i32 to i64 loc(#loc102)
    %18 = arith.muli %16, %17 : i64 loc(#loc102)
    %c2147483647_i64_2 = arith.constant 2147483647 : i64 loc(#loc102)
    %c-2147483648_i64_3 = arith.constant -2147483648 : i64 loc(#loc102)
    %19 = arith.cmpi sle, %18, %c2147483647_i64_2 : i64 loc(#loc102)
    %20 = arith.cmpi sge, %18, %c-2147483648_i64_3 : i64 loc(#loc102)
    %21 = arith.andi %19, %20 : i1 loc(#loc102)
    %22 = arith.muli %1, %arg3 : i32 loc(#loc102)
    %23 = arith.extsi %15 : i32 to i64 loc(#loc103)
    %24 = arith.extsi %22 : i32 to i64 loc(#loc103)
    %25 = arith.addi %23, %24 : i64 loc(#loc103)
    %c2147483647_i64_4 = arith.constant 2147483647 : i64 loc(#loc103)
    %c-2147483648_i64_5 = arith.constant -2147483648 : i64 loc(#loc103)
    %26 = arith.cmpi sle, %25, %c2147483647_i64_4 : i64 loc(#loc103)
    %27 = arith.cmpi sge, %25, %c-2147483648_i64_5 : i64 loc(#loc103)
    %28 = arith.andi %26, %27 : i1 loc(#loc103)
    %29 = arith.addi %15, %22 : i32 loc(#loc103)
    %c256_i32 = arith.constant 256 : i32 loc(#loc104)
    %c256_i32_6 = arith.constant 256 : i32 loc(#loc104)
    %30 = arith.extsi %arg4 : i32 to i64 loc(#loc104)
    %31 = arith.extsi %c256_i32_6 : i32 to i64 loc(#loc104)
    %32 = arith.muli %30, %31 : i64 loc(#loc104)
    %c2147483647_i64_7 = arith.constant 2147483647 : i64 loc(#loc104)
    %c-2147483648_i64_8 = arith.constant -2147483648 : i64 loc(#loc104)
    %33 = arith.cmpi sle, %32, %c2147483647_i64_7 : i64 loc(#loc104)
    %34 = arith.cmpi sge, %32, %c-2147483648_i64_8 : i64 loc(#loc104)
    %35 = arith.andi %33, %34 : i1 loc(#loc104)
    %36 = arith.muli %arg4, %c256_i32_6 : i32 loc(#loc104)
    %37 = arith.extsi %29 : i32 to i64 loc(#loc105)
    %38 = arith.extsi %36 : i32 to i64 loc(#loc105)
    %39 = arith.addi %37, %38 : i64 loc(#loc105)
    %c2147483647_i64_9 = arith.constant 2147483647 : i64 loc(#loc105)
    %c-2147483648_i64_10 = arith.constant -2147483648 : i64 loc(#loc105)
    %40 = arith.cmpi sle, %39, %c2147483647_i64_9 : i64 loc(#loc105)
    %41 = arith.cmpi sge, %39, %c-2147483648_i64_10 : i64 loc(#loc105)
    %42 = arith.andi %40, %41 : i1 loc(#loc105)
    %43 = arith.addi %29, %36 : i32 loc(#loc105)
    tt.return %arg0, %arg1, %arg2, %arg3, %arg4, %arg5, %29, %43 : f32, i32, i32, i32, i32, i32, i32, i32 loc(#loc106)
  ^bb1:  // no predecessors
    %44 = ub.poison : f32 loc(#loc107)
    %45 = ub.poison : i32 loc(#loc107)
    %46 = ub.poison : i32 loc(#loc107)
    %47 = ub.poison : i32 loc(#loc107)
    %48 = ub.poison : i32 loc(#loc107)
    %49 = ub.poison : i32 loc(#loc107)
    %50 = ub.poison : i32 loc(#loc107)
    %51 = ub.poison : i32 loc(#loc107)
    tt.return %44, %45, %46, %47, %48, %49, %50, %51 : f32, i32, i32, i32, i32, i32, i32, i32 loc(#loc107)
  } loc(#loc97)
  tt.func private @"__main__.AttentionProgram.get_fused_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":403:0)) -> (i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc109)
    tt.return %c0_i32, %arg3 : i32, i32 loc(#loc109)
  ^bb1:  // no predecessors
    %0 = ub.poison : i32 loc(#loc110)
    %1 = ub.poison : i32 loc(#loc110)
    tt.return %0, %1 : i32, i32 loc(#loc110)
  } loc(#loc108)
  tt.func private @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0)) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) loc(#loc112)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc113)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc114)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1> loc(#loc115)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc115)
    %4 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc115)
    %5 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %6 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %7 = ub.poison : i32 loc(#loc115)
    %8 = ub.poison : i32 loc(#loc115)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc115)
  } loc(#loc111)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0)) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc117)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32] : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> -> !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1> loc(#loc117)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc118)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc118)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc119)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc119)
    %true = arith.constant true loc(#loc120)
    ttng.wait_barrier %1, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc120)
    tt.return %0, %2 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc121)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1> loc(#loc122)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc122)
    tt.return %3, %4 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc122)
  } loc(#loc116)
  tt.func private @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0)) -> (i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc124)
    %c1_i32 = arith.constant 1 : i32 loc(#loc125)
    %c1_i32_0 = arith.constant 1 : i32 loc(#loc125)
    %0 = arith.xori %arg4, %c1_i32_0 : i32 loc(#loc125)
    tt.return %c0_i32, %0 : i32, i32 loc(#loc126)
  ^bb1:  // no predecessors
    %c1_i32_1 = arith.constant 1 : i32 loc(#loc127)
    %c1_i32_2 = arith.constant 1 : i32 loc(#loc127)
    %1 = arith.extsi %arg3 : i32 to i64 loc(#loc127)
    %2 = arith.extsi %c1_i32_2 : i32 to i64 loc(#loc127)
    %3 = arith.addi %1, %2 : i64 loc(#loc127)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc127)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc127)
    %4 = arith.cmpi sle, %3, %c2147483647_i64 : i64 loc(#loc127)
    %5 = arith.cmpi sge, %3, %c-2147483648_i64 : i64 loc(#loc127)
    %6 = arith.andi %4, %5 : i1 loc(#loc127)
    %7 = arith.addi %arg3, %c1_i32_2 : i32 loc(#loc127)
    %c1_i32_3 = arith.constant 1 : i32 loc(#loc128)
    %8 = arith.cmpi eq, %7, %c1_i32_3 : i32 loc(#loc128)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc129)
    %c0_i32_5 = arith.constant 0 : i32 loc(#loc129)
    %9 = arith.select %8, %c0_i32_5, %7 : i32 loc(#loc129)
    %c1_i32_6 = arith.constant 1 : i32 loc(#loc130)
    %c1_i32_7 = arith.constant 1 : i32 loc(#loc130)
    %10 = arith.xori %arg4, %c1_i32_7 : i32 loc(#loc130)
    %11 = arith.select %8, %10, %arg4 : i32 loc(#loc131)
    tt.return %9, %11 : i32, i32 loc(#loc132)
  ^bb2:  // no predecessors
    %12 = ub.poison : i32 loc(#loc133)
    %13 = ub.poison : i32 loc(#loc133)
    tt.return %12, %13 : i32, i32 loc(#loc133)
  } loc(#loc123)
  tt.func private @"__main__._attn_fwd_correction_rescale____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg4: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg5: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg6: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg7: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg8: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg9: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg13: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0), %arg14: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":721:0)) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg5, %arg6, %arg7, %arg8, %arg9) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc135)
    %1 = tt.call @"__main__._borrow_s_as_alpha____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %arg4) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc136)
    %result = ttng.tmem_load %1 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked> loc(#loc137)
    %true = arith.constant true loc(#loc138)
    ttng.arrive_barrier %0#1, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc138)
    %2 = tt.reshape %result : tensor<128x1xf32, #blocked> -> tensor<128xf32, #linear> loc(#loc139)
    %3 = ttg.convert_layout %2 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc140)
    %4:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg10, %arg11, %arg12, %arg13, %arg14) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc141)
    %5 = ttng.tmem_subslice %4#0 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> -> !ttg.memdesc<128x32xf32, #tmem3, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc142)
    %result_0 = ttng.tmem_load %5 : !ttg.memdesc<128x32xf32, #tmem3, #ttng.tensor_memory, mutable, 2x128x64> -> tensor<128x32xf32, #blocked1> loc(#loc143)
    %6 = tt.expand_dims %3 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xf32, #blocked1> loc(#loc144)
    %7 = tt.call @__main__._mul_f32x2__fp32S128_32SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL__(%result_0, %6) : (tensor<128x32xf32, #blocked1>, tensor<128x1xf32, #blocked1>) -> tensor<128x32xf32, #blocked1> loc(#loc145)
    %true_1 = arith.constant true loc(#loc146)
    ttng.tmem_store %7, %5, %true_1 : tensor<128x32xf32, #blocked1> -> !ttg.memdesc<128x32xf32, #tmem3, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc146)
    %8 = ttng.tmem_subslice %4#0 {N = 32 : i32} : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> -> !ttg.memdesc<128x32xf32, #tmem3, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc142)
    %result_2 = ttng.tmem_load %8 : !ttg.memdesc<128x32xf32, #tmem3, #ttng.tensor_memory, mutable, 2x128x64> -> tensor<128x32xf32, #blocked1> loc(#loc143)
    %9 = tt.expand_dims %3 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xf32, #blocked1> loc(#loc144)
    %10 = tt.call @__main__._mul_f32x2__fp32S128_32SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL__(%result_2, %9) : (tensor<128x32xf32, #blocked1>, tensor<128x1xf32, #blocked1>) -> tensor<128x32xf32, #blocked1> loc(#loc145)
    %true_3 = arith.constant true loc(#loc146)
    ttng.tmem_store %10, %8, %true_3 : tensor<128x32xf32, #blocked1> -> !ttg.memdesc<128x32xf32, #tmem3, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc146)
    %true_4 = arith.constant true loc(#loc147)
    ttng.arrive_barrier %4#1, 1, %true_4 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc147)
    tt.return %0#2, %0#3, %0#4, %0#5, %0#6, %4#2, %4#3, %4#4, %4#5, %4#6 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc148)
  ^bb1:  // no predecessors
    %11 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc149)
    %12 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc149)
    %13 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc149)
    %14 = ub.poison : i32 loc(#loc149)
    %15 = ub.poison : i32 loc(#loc149)
    %16 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc149)
    %17 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc149)
    %18 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc149)
    %19 = ub.poison : i32 loc(#loc149)
    %20 = ub.poison : i32 loc(#loc149)
    tt.return %11, %12, %13, %14, %15, %16, %17, %18, %19, %20 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc149)
  } loc(#loc134)
  tt.func private @"__main__._borrow_s_as_alpha____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":483:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":483:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":483:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":483:0), %arg4: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":483:0)) -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> attributes {noinline = false} {
    %0 = ttng.tmem_subslice %arg4 {N = 64 : i32} : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x1xf32, #tmem4, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc151)
    %1 = ttg.memdesc_reinterpret %0 : !ttg.memdesc<128x1xf32, #tmem4, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc152)
    tt.return %1 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc153)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc154)
    tt.return %2 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc154)
  } loc(#loc150)
  tt.func private @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0)) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) loc(#loc112)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc113)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc114)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc115)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc115)
    %4 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc115)
    %5 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %6 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %7 = ub.poison : i32 loc(#loc115)
    %8 = ub.poison : i32 loc(#loc115)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc115)
  } loc(#loc111)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0)) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc117)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc117)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc118)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc118)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc119)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc119)
    %true = arith.constant true loc(#loc120)
    ttng.wait_barrier %1, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc120)
    tt.return %0, %2 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc121)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc122)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc122)
    tt.return %3, %4 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc122)
  } loc(#loc116)
  tt.func private @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0)) -> (i32, i32) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32 loc(#loc127)
    %c1_i32_0 = arith.constant 1 : i32 loc(#loc127)
    %0 = arith.extsi %arg3 : i32 to i64 loc(#loc127)
    %1 = arith.extsi %c1_i32_0 : i32 to i64 loc(#loc127)
    %2 = arith.addi %0, %1 : i64 loc(#loc127)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc127)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc127)
    %3 = arith.cmpi sle, %2, %c2147483647_i64 : i64 loc(#loc127)
    %4 = arith.cmpi sge, %2, %c-2147483648_i64 : i64 loc(#loc127)
    %5 = arith.andi %3, %4 : i1 loc(#loc127)
    %6 = arith.addi %arg3, %c1_i32_0 : i32 loc(#loc127)
    %c2_i32 = arith.constant 2 : i32 loc(#loc128)
    %7 = arith.cmpi eq, %6, %c2_i32 : i32 loc(#loc128)
    %c0_i32 = arith.constant 0 : i32 loc(#loc129)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc129)
    %8 = arith.select %7, %c0_i32_1, %6 : i32 loc(#loc129)
    %c1_i32_2 = arith.constant 1 : i32 loc(#loc130)
    %c1_i32_3 = arith.constant 1 : i32 loc(#loc130)
    %9 = arith.xori %arg4, %c1_i32_3 : i32 loc(#loc130)
    %10 = arith.select %7, %9, %arg4 : i32 loc(#loc131)
    tt.return %8, %10 : i32, i32 loc(#loc132)
  ^bb1:  // no predecessors
    %11 = ub.poison : i32 loc(#loc133)
    %12 = ub.poison : i32 loc(#loc133)
    tt.return %11, %12 : i32, i32 loc(#loc133)
  } loc(#loc123)
  tt.func private @__main__._mul_f32x2__fp32S128_32SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL__(%arg0: tensor<128x32xf32, #blocked1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0), %arg1: tensor<128x1xf32, #blocked1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0)) -> tensor<128x32xf32, #blocked1> attributes {noinline = false} {
    %0 = tt.broadcast %arg1 : tensor<128x1xf32, #blocked1> -> tensor<128x32xf32, #blocked1> loc(#loc156)
    %1 = tt.broadcast %arg1 : tensor<128x1xf32, #blocked1> -> tensor<128x32xf32, #blocked1> loc(#loc156)
    %2 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %arg0, %1 : tensor<128x32xf32, #blocked1>, tensor<128x32xf32, #blocked1> -> tensor<128x32xf32, #blocked1> loc(#loc156)
    tt.return %2 : tensor<128x32xf32, #blocked1> loc(#loc157)
  ^bb1:  // no predecessors
    %3 = ub.poison : tensor<128x32xf32, #blocked1> loc(#loc158)
    tt.return %3 : tensor<128x32xf32, #blocked1> loc(#loc158)
  } loc(#loc155)
  tt.func private @"__main__._attn_fwd_correction_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_Pfp32___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg4: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg8: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg9: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg10: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg11: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg12: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg13: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg14: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg15: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg16: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg17: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg18: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg19: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg20: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg21: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg22: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg23: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg24: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg25: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg26: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg27: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0), %arg28: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":740:0)) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg14, %arg15, %arg16, %arg17, %arg18) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc160)
    %1:2 = tt.call @"__main__._borrow_s_for_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %arg12) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> (!ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>) loc(#loc161)
    %result = ttng.tmem_load %1#0 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked> loc(#loc162)
    %2 = tt.reshape %result : tensor<128x1xf32, #blocked> -> tensor<128xf32, #linear> loc(#loc163)
    %3 = ttg.convert_layout %2 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc164)
    %result_0 = ttng.tmem_load %1#1 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> -> tensor<128x1xf32, #blocked> loc(#loc165)
    %4 = tt.reshape %result_0 : tensor<128x1xf32, #blocked> -> tensor<128xf32, #linear> loc(#loc166)
    %5 = ttg.convert_layout %4 : tensor<128xf32, #linear> -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc167)
    %true = arith.constant true loc(#loc168)
    ttng.arrive_barrier %0#1, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc168)
    %6:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg19, %arg20, %arg21, %arg22, %arg23) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc169)
    %7:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg24, %arg25, %arg26, %arg27, %arg28) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc170)
    %c1_i32 = arith.constant 1 : i32 loc(#loc171)
    %cst = arith.constant 1.000000e+00 : f32 loc(#loc171)
    %cst_1 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc171)
    %8 = arith.divf %cst_1, %5 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc171)
    %9 = ttng.tmem_subslice %7#0 {N = 0 : i32} : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> -> !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc172)
    %result_2 = ttng.tmem_load %9 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> -> tensor<128x64xf32, #blocked1> loc(#loc173)
    %10 = tt.expand_dims %8 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xf32, #blocked1> loc(#loc174)
    %11 = tt.call @__main__._mul_f32x2__fp32S128_64SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL__(%result_2, %10) : (tensor<128x64xf32, #blocked1>, tensor<128x1xf32, #blocked1>) -> tensor<128x64xf32, #blocked1> loc(#loc175)
    %c0_i32 = arith.constant 0 : i32 loc(#loc176)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc176)
    %12 = ttg.memdesc_subview %6#0[%c0_i32, %c0_i32_3] : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc176)
    %13 = tt.fp_to_fp %11, rounding = rtne : tensor<128x64xf32, #blocked1> -> tensor<128x64xf8E5M2, #blocked1> loc(#loc177)
    ttg.local_store %13, %12 : tensor<128x64xf8E5M2, #blocked1> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc178)
    ttng.fence_async_shared {bCluster = false} loc(#loc179)
    %true_4 = arith.constant true loc(#loc180)
    ttng.arrive_barrier %6#1, 1, %true_4 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc180)
    %true_5 = arith.constant true loc(#loc181)
    ttng.arrive_barrier %7#1, 1, %true_5 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc181)
    %14 = math.log2 %5 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc182)
    %15 = arith.addf %3, %14 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc183)
    %c256_i32 = arith.constant 256 : i32 loc(#loc184)
    %c256_i32_6 = arith.constant 256 : i32 loc(#loc184)
    %16 = arith.extsi %arg8 : i32 to i64 loc(#loc184)
    %17 = arith.extsi %c256_i32_6 : i32 to i64 loc(#loc184)
    %18 = arith.muli %16, %17 : i64 loc(#loc184)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc184)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc184)
    %19 = arith.cmpi sle, %18, %c2147483647_i64 : i64 loc(#loc184)
    %20 = arith.cmpi sge, %18, %c-2147483648_i64 : i64 loc(#loc184)
    %21 = arith.andi %19, %20 : i1 loc(#loc184)
    %22 = arith.muli %arg8, %c256_i32_6 : i32 loc(#loc184)
    %23 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked2> loc(#loc185)
    %24 = tt.splat %22 : i32 -> tensor<128xi32, #blocked2> loc(#loc186)
    %25 = arith.extsi %24 : tensor<128xi32, #blocked2> to tensor<128xi64, #blocked2> loc(#loc186)
    %26 = arith.extsi %23 : tensor<128xi32, #blocked2> to tensor<128xi64, #blocked2> loc(#loc186)
    %27 = arith.addi %25, %26 : tensor<128xi64, #blocked2> loc(#loc186)
    %c2147483647_i64_7 = arith.constant 2147483647 : i64 loc(#loc186)
    %c-2147483648_i64_8 = arith.constant -2147483648 : i64 loc(#loc186)
    %cst_9 = arith.constant dense<2147483647> : tensor<128xi64, #blocked2> loc(#loc186)
    %28 = arith.cmpi sle, %27, %cst_9 : tensor<128xi64, #blocked2> loc(#loc186)
    %cst_10 = arith.constant dense<-2147483648> : tensor<128xi64, #blocked2> loc(#loc186)
    %29 = arith.cmpi sge, %27, %cst_10 : tensor<128xi64, #blocked2> loc(#loc186)
    %30 = arith.andi %28, %29 : tensor<128xi1, #blocked2> loc(#loc186)
    %31 = arith.addi %24, %23 : tensor<128xi32, #blocked2> loc(#loc186)
    %32 = arith.extsi %arg9 : i32 to i64 loc(#loc187)
    %33 = arith.extsi %arg3 : i32 to i64 loc(#loc187)
    %34 = arith.muli %32, %33 : i64 loc(#loc187)
    %c2147483647_i64_11 = arith.constant 2147483647 : i64 loc(#loc187)
    %c-2147483648_i64_12 = arith.constant -2147483648 : i64 loc(#loc187)
    %35 = arith.cmpi sle, %34, %c2147483647_i64_11 : i64 loc(#loc187)
    %36 = arith.cmpi sge, %34, %c-2147483648_i64_12 : i64 loc(#loc187)
    %37 = arith.andi %35, %36 : i1 loc(#loc187)
    %38 = arith.muli %arg9, %arg3 : i32 loc(#loc187)
    %39 = tt.addptr %arg13, %38 : !tt.ptr<f32>, i32 loc(#loc188)
    %40 = tt.splat %39 : !tt.ptr<f32> -> tensor<128x!tt.ptr<f32>, #blocked2> loc(#loc189)
    %41 = tt.addptr %40, %31 : tensor<128x!tt.ptr<f32>, #blocked2>, tensor<128xi32, #blocked2> loc(#loc189)
    %42 = ttg.convert_layout %15 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128xf32, #blocked2> loc(#loc190)
    tt.store %41, %42 : tensor<128x!tt.ptr<f32>, #blocked2> loc(#loc191)
    tt.return %0#2, %0#3, %0#4, %0#5, %0#6, %6#2, %6#3, %6#4, %6#5, %6#6, %7#2, %7#3, %7#4, %7#5, %7#6 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc192)
  ^bb1:  // no predecessors
    %43 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc193)
    %44 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc193)
    %45 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc193)
    %46 = ub.poison : i32 loc(#loc193)
    %47 = ub.poison : i32 loc(#loc193)
    %48 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc193)
    %49 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc193)
    %50 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc193)
    %51 = ub.poison : i32 loc(#loc193)
    %52 = ub.poison : i32 loc(#loc193)
    %53 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc193)
    %54 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc193)
    %55 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc193)
    %56 = ub.poison : i32 loc(#loc193)
    %57 = ub.poison : i32 loc(#loc193)
    tt.return %43, %44, %45, %46, %47, %48, %49, %50, %51, %52, %53, %54, %55, %56, %57 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc193)
  } loc(#loc159)
  tt.func private @"__main__._borrow_s_for_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":490:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":490:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":490:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":490:0), %arg4: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":490:0)) -> (!ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>) attributes {noinline = false} {
    %0 = ttng.tmem_subslice %arg4 {N = 65 : i32} : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x1xf32, #tmem4, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc195)
    %1 = ttng.tmem_subslice %arg4 {N = 66 : i32} : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x1xf32, #tmem4, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc196)
    %2 = ttg.memdesc_reinterpret %0 : !ttg.memdesc<128x1xf32, #tmem4, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc197)
    %3 = ttg.memdesc_reinterpret %1 : !ttg.memdesc<128x1xf32, #tmem4, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc198)
    tt.return %2, %3 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc199)
  ^bb1:  // no predecessors
    %4 = ub.poison : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc200)
    %5 = ub.poison : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc200)
    tt.return %4, %5 : !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc200)
  } loc(#loc194)
  tt.func private @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) loc(#loc202)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc203)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc204)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc205)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc205)
    %4 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc205)
    %5 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %6 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %7 = ub.poison : i32 loc(#loc205)
    %8 = ub.poison : i32 loc(#loc205)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc205)
  } loc(#loc201)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc207)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc207)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc208)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc208)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc209)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc209)
    %true = arith.constant true loc(#loc210)
    ttng.wait_barrier %2, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc210)
    tt.return %0, %1 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc211)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc212)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc212)
    tt.return %3, %4 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc212)
  } loc(#loc206)
  tt.func private @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0)) -> (i32, i32) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32 loc(#loc127)
    %c1_i32_0 = arith.constant 1 : i32 loc(#loc127)
    %0 = arith.extsi %arg3 : i32 to i64 loc(#loc127)
    %1 = arith.extsi %c1_i32_0 : i32 to i64 loc(#loc127)
    %2 = arith.addi %0, %1 : i64 loc(#loc127)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc127)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc127)
    %3 = arith.cmpi sle, %2, %c2147483647_i64 : i64 loc(#loc127)
    %4 = arith.cmpi sge, %2, %c-2147483648_i64 : i64 loc(#loc127)
    %5 = arith.andi %3, %4 : i1 loc(#loc127)
    %6 = arith.addi %arg3, %c1_i32_0 : i32 loc(#loc127)
    %c2_i32 = arith.constant 2 : i32 loc(#loc128)
    %7 = arith.cmpi eq, %6, %c2_i32 : i32 loc(#loc128)
    %c0_i32 = arith.constant 0 : i32 loc(#loc129)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc129)
    %8 = arith.select %7, %c0_i32_1, %6 : i32 loc(#loc129)
    %c1_i32_2 = arith.constant 1 : i32 loc(#loc130)
    %c1_i32_3 = arith.constant 1 : i32 loc(#loc130)
    %9 = arith.xori %arg4, %c1_i32_3 : i32 loc(#loc130)
    %10 = arith.select %7, %9, %arg4 : i32 loc(#loc131)
    tt.return %8, %10 : i32, i32 loc(#loc132)
  ^bb1:  // no predecessors
    %11 = ub.poison : i32 loc(#loc133)
    %12 = ub.poison : i32 loc(#loc133)
    tt.return %11, %12 : i32, i32 loc(#loc133)
  } loc(#loc123)
  tt.func private @__main__._mul_f32x2__fp32S128_64SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_32B32_1B4_1B0_1B1_1B1_1B1_0BL__(%arg0: tensor<128x64xf32, #blocked1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0), %arg1: tensor<128x1xf32, #blocked1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0)) -> tensor<128x64xf32, #blocked1> attributes {noinline = false} {
    %0 = tt.broadcast %arg1 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1> loc(#loc156)
    %1 = tt.broadcast %arg1 : tensor<128x1xf32, #blocked1> -> tensor<128x64xf32, #blocked1> loc(#loc156)
    %2 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %arg0, %1 : tensor<128x64xf32, #blocked1>, tensor<128x64xf32, #blocked1> -> tensor<128x64xf32, #blocked1> loc(#loc156)
    tt.return %2 : tensor<128x64xf32, #blocked1> loc(#loc157)
  ^bb1:  // no predecessors
    %3 = ub.poison : tensor<128x64xf32, #blocked1> loc(#loc158)
    tt.return %3 : tensor<128x64xf32, #blocked1> loc(#loc158)
  } loc(#loc155)
  tt.func private @"__main__._attn_fwd_softmax0____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg4: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg5: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg6: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg7: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg8: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg9: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg13: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg14: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg15: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg16: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg17: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg19: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg20: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg21: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg22: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg23: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg24: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg25: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg28: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg29: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg30: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg31: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg32: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg33: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg34: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg35: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg36: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg37: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg38: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg39: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg40: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg41: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg42: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg43: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg44: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg45: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg46: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg47: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0), %arg48: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":685:0)) attributes {noinline = false} {
    tt.call @"__main__._softmax_tile____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_Pfp32_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__(0,)cconstexpr_0__(4,)cconstexpr_1_"(%arg0, %arg1, %arg2, %arg3, %arg48, %arg43, %arg44, %arg45, %arg46, %arg47, %arg16, %arg17, %arg18, %arg22, %arg23, %arg24) : (f32, i32, i32, i32, !tt.ptr<f32>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc214)
    tt.return loc(#loc215)
  } loc(#loc213)
  tt.func private @"__main__._softmax_tile____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_Pfp32_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__(0,)cconstexpr_0__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg4: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg5: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg8: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg9: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg10: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg11: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg12: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg13: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg14: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg15: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0)) attributes {noinline = false} {
    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> loc(#loc217)
    %1:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg10, %arg11, %arg12) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc218)
    %2:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg13, %arg14, %arg15) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc219)
    %3:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%2#0, %2#1, %2#2, %2#3, %2#4) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc220)
    %4:8 = tt.call @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0, %arg1, %arg2, %arg3) : (f32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc221)
    %c78_i32 = arith.constant 78 : i32 loc(#loc222)
    %5 = arith.bitcast %4#4 : i32 to i32 loc(#loc222)
    %6 = arith.bitcast %4#7 : i32 to i32 loc(#loc222)
    %7 = arith.bitcast %c78_i32 : i32 to i32 loc(#loc222)
    %8 = ub.poison : i32 loc(#loc222)
    %9:12 = scf.for %arg16 = %5 to %6 step %7 iter_args(%arg17 = %1#0, %arg18 = %1#1, %arg19 = %1#2, %arg20 = %1#3, %arg21 = %1#4, %arg22 = %3#2, %arg23 = %3#3, %arg24 = %3#4, %arg25 = %3#5, %arg26 = %3#6, %arg27 = %3#0, %arg28 = %3#1) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>)  : i32 {
      %10:8 = tt.call @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%4#0, %4#1, %4#2, %4#3, %4#4, %4#5, %4#6, %4#7, %arg16) : (f32, i32, i32, i32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc223)
      %c256_i32 = arith.constant 256 : i32 loc(#loc224)
      %c256_i32_0 = arith.constant 256 : i32 loc(#loc224)
      %11 = arith.extsi %10#4 : i32 to i64 loc(#loc224)
      %12 = arith.extsi %c256_i32_0 : i32 to i64 loc(#loc224)
      %13 = arith.muli %11, %12 : i64 loc(#loc224)
      %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc224)
      %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc224)
      %14 = arith.cmpi sle, %13, %c2147483647_i64 : i64 loc(#loc224)
      %15 = arith.cmpi sge, %13, %c-2147483648_i64 : i64 loc(#loc224)
      %16 = arith.andi %14, %15 : i1 loc(#loc224)
      %17 = arith.muli %10#4, %c256_i32_0 : i32 loc(#loc224)
      %18 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc225)
      %19 = tt.splat %17 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %20 = arith.extsi %19 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %21 = arith.extsi %18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %22 = arith.addi %20, %21 : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %c2147483647_i64_1 = arith.constant 2147483647 : i64 loc(#loc226)
      %c-2147483648_i64_2 = arith.constant -2147483648 : i64 loc(#loc226)
      %cst = arith.constant dense<2147483647> : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %23 = arith.cmpi sle, %22, %cst : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %cst_3 = arith.constant dense<-2147483648> : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %24 = arith.cmpi sge, %22, %cst_3 : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %25 = arith.andi %23, %24 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %26 = arith.addi %19, %18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %cst_4 = arith.constant 0xFF800000 : f32 loc(#loc227)
      %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc227)
      %cst_6 = arith.constant 1.000000e+00 : f32 loc(#loc228)
      %cst_7 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc228)
      %27:13 = tt.call @"__main__._softmax_inner_loop____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_i32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_i32S128SLSL0_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL__(0,)cconstexpr_0__(10,)cconstexpr_3_"(%arg0, %arg1, %arg2, %arg3, %10#0, %10#1, %10#2, %10#3, %10#4, %10#5, %10#6, %10#7, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg28, %26, %0, %cst_5, %cst_7) : (f32, i32, i32, i32, f32, i32, i32, i32, i32, i32, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc229)
      %28:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%27#3, %27#4, %27#5, %27#6, %27#7) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc230)
      %29:2 = tt.call @"__main__._borrow_s_for_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %28#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> (!ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>) loc(#loc231)
      %30 = tt.expand_dims %27#0 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc232)
      %31 = ttg.convert_layout %30 : tensor<128x1xf32, #blocked3> -> tensor<128x1xf32, #blocked> loc(#loc233)
      %true = arith.constant true loc(#loc234)
      ttng.tmem_store %31, %29#0, %true : tensor<128x1xf32, #blocked> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc234)
      %32 = tt.expand_dims %27#1 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc235)
      %33 = ttg.convert_layout %32 : tensor<128x1xf32, #blocked3> -> tensor<128x1xf32, #blocked> loc(#loc236)
      %true_8 = arith.constant true loc(#loc237)
      ttng.tmem_store %33, %29#1, %true_8 : tensor<128x1xf32, #blocked> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc237)
      %true_9 = arith.constant true loc(#loc238)
      ttng.arrive_barrier %27#2, 1, %true_9 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc238)
      %34:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%27#8, %27#9, %27#10, %27#11, %27#12) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc239)
      %true_10 = arith.constant true loc(#loc240)
      ttng.arrive_barrier %28#1, 1, %true_10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc240)
      scf.yield %28#2, %28#3, %28#4, %28#5, %28#6, %34#2, %34#3, %34#4, %34#5, %34#6, %34#0, %34#1 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc241)
    } loc(#loc222)
    tt.return loc(#loc242)
  } loc(#loc216)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0)) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc62)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc63)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc64)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc65)
    %1 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %2 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %3 = ub.poison : i32 loc(#loc65)
    %4 = ub.poison : i32 loc(#loc65)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc65)
  } loc(#loc61)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0)) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc67)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc68)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc69)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc70)
    %1 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %2 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %3 = ub.poison : i32 loc(#loc70)
    %4 = ub.poison : i32 loc(#loc70)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc70)
  } loc(#loc66)
  tt.func private @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0)) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) loc(#loc202)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc203)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc204)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1> loc(#loc205)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc205)
    %4 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc205)
    %5 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %6 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %7 = ub.poison : i32 loc(#loc205)
    %8 = ub.poison : i32 loc(#loc205)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc205)
  } loc(#loc201)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0)) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc207)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32] : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> -> !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1> loc(#loc207)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc208)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc208)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc209)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc209)
    %true = arith.constant true loc(#loc210)
    ttng.wait_barrier %2, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc210)
    tt.return %0, %1 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc211)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1> loc(#loc212)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc212)
    tt.return %3, %4 : !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc212)
  } loc(#loc206)
  tt.func private @"__main__._softmax_inner_loop____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_i32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_i32S128SLSL0_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL__(0,)cconstexpr_0__(10,)cconstexpr_3_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg4: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg8: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg9: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg10: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg11: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg12: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg13: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg14: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg15: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg16: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg17: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg19: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg20: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg21: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg22: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg23: tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg24: tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg25: tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg26: tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0)) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.AttentionProgram.get_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_3_"(%arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11) : (f32, i32, i32, i32, i32, i32, i32, i32) -> (i32, i32) loc(#loc244)
    %c128_i32 = arith.constant 128 : i32 loc(#loc245)
    %1 = arith.bitcast %0#0 : i32 to i32 loc(#loc245)
    %2 = arith.bitcast %0#1 : i32 to i32 loc(#loc245)
    %3 = arith.bitcast %c128_i32 : i32 to i32 loc(#loc245)
    %4 = ub.poison : i32 loc(#loc245)
    %5:13 = scf.for %arg27 = %1 to %2 step %3 iter_args(%arg28 = %arg12, %arg29 = %arg13, %arg30 = %arg14, %arg31 = %arg15, %arg32 = %arg16, %arg33 = %arg17, %arg34 = %arg18, %arg35 = %arg19, %arg36 = %arg20, %arg37 = %arg21, %arg38 = %arg22, %arg39 = %arg25, %arg40 = %arg26) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>)  : i32 {
      %19:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg28, %arg29, %arg30, %arg31, %arg32) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc246)
      %result = ttng.tmem_load %19#0 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> tensor<128x128xf32, #blocked3> loc(#loc247)
      %20 = tt.call @"triton.language.standard.max__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(1,)cconstexpr_1__(2,)cconstexpr_False__(3,)cconstexpr_True__(4,)cconstexpr_False_"(%result) : (tensor<128x128xf32, #blocked3>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc248)
      %21 = tt.splat %arg0 : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc249)
      %22 = arith.mulf %20, %21 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc249)
      %23 = arith.maxnumf %arg39, %22 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc250)
      %24 = arith.subf %arg39, %23 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc251)
      %25 = math.exp2 %24 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc252)
      %26 = tt.call @"__main__._borrow_s_as_alpha____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %19#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc253)
      %27 = tt.expand_dims %25 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc254)
      %28 = ttg.convert_layout %27 : tensor<128x1xf32, #blocked3> -> tensor<128x1xf32, #blocked> loc(#loc255)
      %true = arith.constant true loc(#loc256)
      ttng.tmem_store %28, %26, %true : tensor<128x1xf32, #blocked> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc256)
      %true_0 = arith.constant true loc(#loc257)
      ttng.arrive_barrier %arg38, 1, %true_0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc257)
      %29 = tt.call @"triton.experimental.gluon.language._standard.full_like__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32__(2,)cNone_(3,)cNone_(4,)cNone"(%result, %arg0) : (tensor<128x128xf32, #blocked3>, f32) -> tensor<128x128xf32, #blocked3> loc(#loc258)
      %30 = tt.call @__main__._mul_f32x2__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(%result, %29) : (tensor<128x128xf32, #blocked3>, tensor<128x128xf32, #blocked3>) -> tensor<128x128xf32, #blocked3> loc(#loc259)
      %31 = tt.expand_dims %23 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc260)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc261)
      %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x1xf32, #blocked3> loc(#loc261)
      %32 = arith.subf %cst_1, %31 : tensor<128x1xf32, #blocked3> loc(#loc261)
      %33 = tt.call @__main__._add_f32x2__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(%30, %32) : (tensor<128x128xf32, #blocked3>, tensor<128x1xf32, #blocked3>) -> tensor<128x128xf32, #blocked3> loc(#loc262)
      %34 = tt.reshape %33 : tensor<128x128xf32, #blocked3> -> tensor<128x2x64xf32, #blocked4> loc(#loc263)
      %35 = tt.trans %34 {order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc264)
      %outLHS, %outRHS = tt.split %35 : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc265)
      %36 = tt.call @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %19#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc266)
      %37 = math.exp2 %outLHS : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc267)
      %38 = ttng.tmem_subslice %36 {N = 0 : i32} : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc268)
      %39 = tt.fp_to_fp %37, rounding = rtne : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> -> tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc269)
      %true_2 = arith.constant true loc(#loc270)
      ttng.tmem_store %39, %38, %true_2 : tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc270)
      %40 = math.exp2 %outRHS : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc271)
      %41 = ttng.tmem_subslice %36 {N = 64 : i32} : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc272)
      %42 = tt.fp_to_fp %40, rounding = rtne : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> -> tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc273)
      %true_3 = arith.constant true loc(#loc274)
      ttng.tmem_store %42, %41, %true_3 : tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc274)
      %true_4 = arith.constant true loc(#loc275)
      ttng.arrive_barrier %19#1, 1, %true_4 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc275)
      %43:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg33, %arg34, %arg35, %arg36, %arg37) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc276)
      %44 = tt.join %37, %40 : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> -> tensor<128x64x2xf32, #blocked5> loc(#loc277)
      %45 = tt.trans %44 {order = array<i32: 0, 2, 1>} : tensor<128x64x2xf32, #blocked5> -> tensor<128x2x64xf32, #blocked4> loc(#loc278)
      %46 = tt.reshape %45 : tensor<128x2x64xf32, #blocked4> -> tensor<128x128xf32, #blocked3> loc(#loc279)
      %47 = ttg.convert_layout %46 : tensor<128x128xf32, #blocked3> -> tensor<128x128xf32, #blocked3> loc(#loc280)
      %48 = tt.call @"triton.language.standard.sum__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(1,)cconstexpr_1__(2,)cconstexpr_False__(3,)cNone"(%47) : (tensor<128x128xf32, #blocked3>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc281)
      %49 = arith.mulf %arg40, %25 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc282)
      %50 = arith.addf %49, %48 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc283)
      scf.yield %19#2, %19#3, %19#4, %19#5, %19#6, %43#2, %43#3, %43#4, %43#5, %43#6, %43#1, %23, %50 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc284)
    } loc(#loc245)
    tt.return %5#11, %5#12, %5#10, %5#0, %5#1, %5#2, %5#3, %5#4, %5#5, %5#6, %5#7, %5#8, %5#9 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc285)
  ^bb1:  // no predecessors
    %6 = ub.poison : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc286)
    %7 = ub.poison : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc286)
    %8 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc286)
    %9 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc286)
    %10 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %11 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %12 = ub.poison : i32 loc(#loc286)
    %13 = ub.poison : i32 loc(#loc286)
    %14 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc286)
    %15 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %16 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %17 = ub.poison : i32 loc(#loc286)
    %18 = ub.poison : i32 loc(#loc286)
    tt.return %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc286)
  } loc(#loc243)
  tt.func private @"__main__.AttentionProgram.get_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_3_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":415:0)) -> (i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc288)
    tt.return %c0_i32, %arg3 : i32, i32 loc(#loc289)
  ^bb1:  // no predecessors
    %0 = ub.poison : i32 loc(#loc290)
    %1 = ub.poison : i32 loc(#loc290)
    tt.return %0, %1 : i32, i32 loc(#loc290)
  } loc(#loc287)
  tt.func private @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0)) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) loc(#loc112)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc113)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc114)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc115)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc115)
    %4 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc115)
    %5 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %6 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %7 = ub.poison : i32 loc(#loc115)
    %8 = ub.poison : i32 loc(#loc115)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc115)
  } loc(#loc111)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0)) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc117)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc117)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc118)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc118)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc119)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc119)
    %true = arith.constant true loc(#loc120)
    ttng.wait_barrier %1, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc120)
    tt.return %0, %2 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc121)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc122)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc122)
    tt.return %3, %4 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc122)
  } loc(#loc116)
  tt.func private @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0)) -> (i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc124)
    %c1_i32 = arith.constant 1 : i32 loc(#loc125)
    %c1_i32_0 = arith.constant 1 : i32 loc(#loc125)
    %0 = arith.xori %arg4, %c1_i32_0 : i32 loc(#loc125)
    tt.return %c0_i32, %0 : i32, i32 loc(#loc126)
  ^bb1:  // no predecessors
    %c1_i32_1 = arith.constant 1 : i32 loc(#loc127)
    %c1_i32_2 = arith.constant 1 : i32 loc(#loc127)
    %1 = arith.extsi %arg3 : i32 to i64 loc(#loc127)
    %2 = arith.extsi %c1_i32_2 : i32 to i64 loc(#loc127)
    %3 = arith.addi %1, %2 : i64 loc(#loc127)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc127)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc127)
    %4 = arith.cmpi sle, %3, %c2147483647_i64 : i64 loc(#loc127)
    %5 = arith.cmpi sge, %3, %c-2147483648_i64 : i64 loc(#loc127)
    %6 = arith.andi %4, %5 : i1 loc(#loc127)
    %7 = arith.addi %arg3, %c1_i32_2 : i32 loc(#loc127)
    %c1_i32_3 = arith.constant 1 : i32 loc(#loc128)
    %8 = arith.cmpi eq, %7, %c1_i32_3 : i32 loc(#loc128)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc129)
    %c0_i32_5 = arith.constant 0 : i32 loc(#loc129)
    %9 = arith.select %8, %c0_i32_5, %7 : i32 loc(#loc129)
    %c1_i32_6 = arith.constant 1 : i32 loc(#loc130)
    %c1_i32_7 = arith.constant 1 : i32 loc(#loc130)
    %10 = arith.xori %arg4, %c1_i32_7 : i32 loc(#loc130)
    %11 = arith.select %8, %10, %arg4 : i32 loc(#loc131)
    tt.return %9, %11 : i32, i32 loc(#loc132)
  ^bb2:  // no predecessors
    %12 = ub.poison : i32 loc(#loc133)
    %13 = ub.poison : i32 loc(#loc133)
    tt.return %12, %13 : i32, i32 loc(#loc133)
  } loc(#loc123)
  tt.func private @"triton.language.standard.max__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(1,)cconstexpr_1__(2,)cconstexpr_False__(3,)cconstexpr_True__(4,)cconstexpr_False_"(%arg0: tensor<128x128xf32, #blocked3> loc("/root/triton/python/triton/language/standard.py":174:0)) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> attributes {noinline = false} {
    %0 = "tt.reduce"(%arg0) <{axis = 1 : i32}> ({
    ^bb0(%arg1: f32 loc(unknown), %arg2: f32 loc(unknown)):
      %2 = tt.call @triton.language.standard._elementwise_max__fp32_fp32__(%arg1, %arg2) : (f32, f32) -> f32 loc(#loc292)
      tt.reduce.return %2 : f32 loc(#loc292)
    }) : (tensor<128x128xf32, #blocked3>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc292)
    tt.return %0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc293)
  ^bb1:  // no predecessors
    %1 = ub.poison : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc294)
    tt.return %1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc294)
  } loc(#loc291)
  tt.func private @triton.language.standard._elementwise_max__fp32_fp32__(%arg0: f32 loc("/root/triton/python/triton/language/standard.py":166:0), %arg1: f32 loc("/root/triton/python/triton/language/standard.py":166:0)) -> f32 attributes {noinline = false} {
    %0 = arith.maxnumf %arg0, %arg1 : f32 loc(#loc296)
    tt.return %0 : f32 loc(#loc297)
  ^bb1:  // no predecessors
    %1 = ub.poison : f32 loc(#loc298)
    tt.return %1 : f32 loc(#loc298)
  } loc(#loc295)
  tt.func private @"triton.experimental.gluon.language._standard.full_like__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32__(2,)cNone_(3,)cNone_(4,)cNone"(%arg0: tensor<128x128xf32, #blocked3> loc("/root/triton/python/triton/experimental/gluon/language/_standard.py":51:0), %arg1: f32 loc("/root/triton/python/triton/experimental/gluon/language/_standard.py":51:0)) -> tensor<128x128xf32, #blocked3> attributes {noinline = false} {
    %0 = tt.splat %arg1 : f32 -> tensor<128x128xf32, #blocked3> loc(#loc300)
    tt.return %0 : tensor<128x128xf32, #blocked3> loc(#loc301)
  ^bb1:  // no predecessors
    %1 = ub.poison : tensor<128x128xf32, #blocked3> loc(#loc302)
    tt.return %1 : tensor<128x128xf32, #blocked3> loc(#loc302)
  } loc(#loc299)
  tt.func private @__main__._mul_f32x2__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(%arg0: tensor<128x128xf32, #blocked3> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0), %arg1: tensor<128x128xf32, #blocked3> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":452:0)) -> tensor<128x128xf32, #blocked3> attributes {noinline = false} {
    %0 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            mul.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %arg0, %arg1 : tensor<128x128xf32, #blocked3>, tensor<128x128xf32, #blocked3> -> tensor<128x128xf32, #blocked3> loc(#loc156)
    tt.return %0 : tensor<128x128xf32, #blocked3> loc(#loc157)
  ^bb1:  // no predecessors
    %1 = ub.poison : tensor<128x128xf32, #blocked3> loc(#loc158)
    tt.return %1 : tensor<128x128xf32, #blocked3> loc(#loc158)
  } loc(#loc155)
  tt.func private @__main__._add_f32x2__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(%arg0: tensor<128x128xf32, #blocked3> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":432:0), %arg1: tensor<128x1xf32, #blocked3> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":432:0)) -> tensor<128x128xf32, #blocked3> attributes {noinline = false} {
    %0 = tt.broadcast %arg1 : tensor<128x1xf32, #blocked3> -> tensor<128x128xf32, #blocked3> loc(#loc304)
    %1 = tt.broadcast %arg1 : tensor<128x1xf32, #blocked3> -> tensor<128x128xf32, #blocked3> loc(#loc304)
    %2 = tt.elementwise_inline_asm "\0A        {\0A            .reg .b64 ra, rb, rc;\0A            mov.b64 ra, { $2, $3 };\0A            mov.b64 rb, { $4, $5 };\0A            add.f32x2 rc, ra, rb;\0A            mov.b64 { $0, $1 }, rc;\0A        }\0A        " {constraints = "=r,=r,r,r,r,r", packed_element = 2 : i32, pure = true} %arg0, %1 : tensor<128x128xf32, #blocked3>, tensor<128x128xf32, #blocked3> -> tensor<128x128xf32, #blocked3> loc(#loc304)
    tt.return %2 : tensor<128x128xf32, #blocked3> loc(#loc305)
  ^bb1:  // no predecessors
    %3 = ub.poison : tensor<128x128xf32, #blocked3> loc(#loc306)
    tt.return %3 : tensor<128x128xf32, #blocked3> loc(#loc306)
  } loc(#loc303)
  tt.func private @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":477:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":477:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":477:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":477:0), %arg4: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":477:0)) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> attributes {noinline = false} {
    %0 = ttng.tmem_subslice %arg4 {N = 0 : i32} : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc308)
    %1 = ttg.memdesc_reinterpret %0 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 1x128x128> -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc309)
    tt.return %1 : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc310)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc311)
    tt.return %2 : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc311)
  } loc(#loc307)
  tt.func private @"triton.language.standard.sum__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(1,)cconstexpr_1__(2,)cconstexpr_False__(3,)cNone"(%arg0: tensor<128x128xf32, #blocked3> loc("/root/triton/python/triton/language/standard.py":284:0)) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> attributes {noinline = false} {
    %0 = "tt.reduce"(%arg0) <{axis = 1 : i32}> ({
    ^bb0(%arg1: f32 loc(unknown), %arg2: f32 loc(unknown)):
      %2 = tt.call @triton.language.standard._sum_combine__fp32_fp32__(%arg1, %arg2) : (f32, f32) -> f32 loc(#loc313)
      tt.reduce.return %2 : f32 loc(#loc313)
    }) : (tensor<128x128xf32, #blocked3>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc313)
    tt.return %0 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc314)
  ^bb1:  // no predecessors
    %1 = ub.poison : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc315)
    tt.return %1 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc315)
  } loc(#loc312)
  tt.func private @triton.language.standard._sum_combine__fp32_fp32__(%arg0: f32 loc("/root/triton/python/triton/language/standard.py":259:0), %arg1: f32 loc("/root/triton/python/triton/language/standard.py":259:0)) -> f32 attributes {noinline = false} {
    %0 = arith.addf %arg0, %arg1 : f32 loc(#loc317)
    tt.return %0 : f32 loc(#loc318)
  ^bb1:  // no predecessors
    %1 = ub.poison : f32 loc(#loc319)
    tt.return %1 : f32 loc(#loc319)
  } loc(#loc316)
  tt.func private @"__main__._attn_fwd_softmax1____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg4: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg5: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg6: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg7: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg8: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg9: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg13: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg14: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg15: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg16: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg17: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg19: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg20: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg21: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg22: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg23: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg24: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg25: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg28: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg29: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg30: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg31: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg32: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg33: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg34: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg35: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg36: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg37: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg38: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg39: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg40: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg41: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg42: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg43: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg44: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg45: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg46: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg47: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0), %arg48: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":692:0)) attributes {noinline = false} {
    tt.call @"__main__._softmax_tile____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_Pfp32_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__(0,)cconstexpr_1__(4,)cconstexpr_1_"(%arg0, %arg1, %arg2, %arg3, %arg48, %arg43, %arg44, %arg45, %arg46, %arg47, %arg19, %arg20, %arg21, %arg25, %arg26, %arg27) : (f32, i32, i32, i32, !tt.ptr<f32>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> () loc(#loc321)
    tt.return loc(#loc322)
  } loc(#loc320)
  tt.func private @"__main__._softmax_tile____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_Pfp32_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__(0,)cconstexpr_1__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg4: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg5: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg8: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg9: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg10: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg11: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg12: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg13: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg14: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0), %arg15: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":643:0)) attributes {noinline = false} {
    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> loc(#loc217)
    %1:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg10, %arg11, %arg12) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc218)
    %2:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg13, %arg14, %arg15) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc219)
    %3:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%2#0, %2#1, %2#2, %2#3, %2#4) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc220)
    %4:8 = tt.call @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0, %arg1, %arg2, %arg3) : (f32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc221)
    %c78_i32 = arith.constant 78 : i32 loc(#loc222)
    %5 = arith.bitcast %4#4 : i32 to i32 loc(#loc222)
    %6 = arith.bitcast %4#7 : i32 to i32 loc(#loc222)
    %7 = arith.bitcast %c78_i32 : i32 to i32 loc(#loc222)
    %8 = ub.poison : i32 loc(#loc222)
    %9:12 = scf.for %arg16 = %5 to %6 step %7 iter_args(%arg17 = %1#0, %arg18 = %1#1, %arg19 = %1#2, %arg20 = %1#3, %arg21 = %1#4, %arg22 = %3#2, %arg23 = %3#3, %arg24 = %3#4, %arg25 = %3#5, %arg26 = %3#6, %arg27 = %3#0, %arg28 = %3#1) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>)  : i32 {
      %10:8 = tt.call @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%4#0, %4#1, %4#2, %4#3, %4#4, %4#5, %4#6, %4#7, %arg16) : (f32, i32, i32, i32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc223)
      %c256_i32 = arith.constant 256 : i32 loc(#loc224)
      %c256_i32_0 = arith.constant 256 : i32 loc(#loc224)
      %11 = arith.extsi %10#4 : i32 to i64 loc(#loc224)
      %12 = arith.extsi %c256_i32_0 : i32 to i64 loc(#loc224)
      %13 = arith.muli %11, %12 : i64 loc(#loc224)
      %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc224)
      %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc224)
      %14 = arith.cmpi sle, %13, %c2147483647_i64 : i64 loc(#loc224)
      %15 = arith.cmpi sge, %13, %c-2147483648_i64 : i64 loc(#loc224)
      %16 = arith.andi %14, %15 : i1 loc(#loc224)
      %17 = arith.muli %10#4, %c256_i32_0 : i32 loc(#loc224)
      %18 = tt.make_range {end = 256 : i32, start = 128 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc225)
      %19 = tt.splat %17 : i32 -> tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %20 = arith.extsi %19 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %21 = arith.extsi %18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> to tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %22 = arith.addi %20, %21 : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %c2147483647_i64_1 = arith.constant 2147483647 : i64 loc(#loc226)
      %c-2147483648_i64_2 = arith.constant -2147483648 : i64 loc(#loc226)
      %cst = arith.constant dense<2147483647> : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %23 = arith.cmpi sle, %22, %cst : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %cst_3 = arith.constant dense<-2147483648> : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %24 = arith.cmpi sge, %22, %cst_3 : tensor<128xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %25 = arith.andi %23, %24 : tensor<128xi1, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %26 = arith.addi %19, %18 : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc226)
      %cst_4 = arith.constant 0xFF800000 : f32 loc(#loc227)
      %cst_5 = arith.constant dense<0xFF800000> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc227)
      %cst_6 = arith.constant 1.000000e+00 : f32 loc(#loc228)
      %cst_7 = arith.constant dense<1.000000e+00> : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc228)
      %27:13 = tt.call @"__main__._softmax_inner_loop____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_i32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_i32S128SLSL0_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL__(0,)cconstexpr_1__(10,)cconstexpr_3_"(%arg0, %arg1, %arg2, %arg3, %10#0, %10#1, %10#2, %10#3, %10#4, %10#5, %10#6, %10#7, %arg17, %arg18, %arg19, %arg20, %arg21, %arg22, %arg23, %arg24, %arg25, %arg26, %arg28, %26, %0, %cst_5, %cst_7) : (f32, i32, i32, i32, f32, i32, i32, i32, i32, i32, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc229)
      %28:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%27#3, %27#4, %27#5, %27#6, %27#7) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc230)
      %29:2 = tt.call @"__main__._borrow_s_for_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %28#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> (!ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable>) loc(#loc231)
      %30 = tt.expand_dims %27#0 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc232)
      %31 = ttg.convert_layout %30 : tensor<128x1xf32, #blocked3> -> tensor<128x1xf32, #blocked> loc(#loc233)
      %true = arith.constant true loc(#loc234)
      ttng.tmem_store %31, %29#0, %true : tensor<128x1xf32, #blocked> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc234)
      %32 = tt.expand_dims %27#1 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc235)
      %33 = ttg.convert_layout %32 : tensor<128x1xf32, #blocked3> -> tensor<128x1xf32, #blocked> loc(#loc236)
      %true_8 = arith.constant true loc(#loc237)
      ttng.tmem_store %33, %29#1, %true_8 : tensor<128x1xf32, #blocked> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc237)
      %true_9 = arith.constant true loc(#loc238)
      ttng.arrive_barrier %27#2, 1, %true_9 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc238)
      %34:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%27#8, %27#9, %27#10, %27#11, %27#12) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc239)
      %true_10 = arith.constant true loc(#loc240)
      ttng.arrive_barrier %28#1, 1, %true_10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc240)
      scf.yield %28#2, %28#3, %28#4, %28#5, %28#6, %34#2, %34#3, %34#4, %34#5, %34#6, %34#0, %34#1 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc241)
    } loc(#loc222)
    tt.return loc(#loc242)
  } loc(#loc216)
  tt.func private @"__main__._softmax_inner_loop____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>___main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>___main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_i32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_i32S128SLSL0_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL_fp32S128SLSL1_B1_128B32_1B4_1B0_1B1_1B1_1B1_0BSLL__(0,)cconstexpr_1__(10,)cconstexpr_3_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg4: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg5: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg6: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg8: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg9: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg10: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg11: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg12: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg13: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg14: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg15: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg16: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg17: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg19: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg20: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg21: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg22: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg23: tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg24: tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg25: tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0), %arg26: tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":598:0)) -> (tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.AttentionProgram.get_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_3_"(%arg4, %arg5, %arg6, %arg7, %arg8, %arg9, %arg10, %arg11) : (f32, i32, i32, i32, i32, i32, i32, i32) -> (i32, i32) loc(#loc244)
    %c128_i32 = arith.constant 128 : i32 loc(#loc245)
    %1 = arith.bitcast %0#0 : i32 to i32 loc(#loc245)
    %2 = arith.bitcast %0#1 : i32 to i32 loc(#loc245)
    %3 = arith.bitcast %c128_i32 : i32 to i32 loc(#loc245)
    %4 = ub.poison : i32 loc(#loc245)
    %5:13 = scf.for %arg27 = %1 to %2 step %3 iter_args(%arg28 = %arg12, %arg29 = %arg13, %arg30 = %arg14, %arg31 = %arg15, %arg32 = %arg16, %arg33 = %arg17, %arg34 = %arg18, %arg35 = %arg19, %arg36 = %arg20, %arg37 = %arg21, %arg38 = %arg22, %arg39 = %arg25, %arg40 = %arg26) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>)  : i32 {
      %19:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg28, %arg29, %arg30, %arg31, %arg32) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc246)
      %result = ttng.tmem_load %19#0 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> -> tensor<128x128xf32, #blocked3> loc(#loc247)
      %20 = tt.call @"triton.language.standard.max__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(1,)cconstexpr_1__(2,)cconstexpr_False__(3,)cconstexpr_True__(4,)cconstexpr_False_"(%result) : (tensor<128x128xf32, #blocked3>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc248)
      %21 = tt.splat %arg0 : f32 -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc249)
      %22 = arith.mulf %20, %21 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc249)
      %23 = arith.maxnumf %arg39, %22 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc250)
      %24 = arith.subf %arg39, %23 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc251)
      %25 = math.exp2 %24 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc252)
      %26 = tt.call @"__main__._borrow_s_as_alpha____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %19#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc253)
      %27 = tt.expand_dims %25 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc254)
      %28 = ttg.convert_layout %27 : tensor<128x1xf32, #blocked3> -> tensor<128x1xf32, #blocked> loc(#loc255)
      %true = arith.constant true loc(#loc256)
      ttng.tmem_store %28, %26, %true : tensor<128x1xf32, #blocked> -> !ttg.memdesc<128x1xf32, #tmem2, #ttng.tensor_memory, mutable> loc(#loc256)
      %true_0 = arith.constant true loc(#loc257)
      ttng.arrive_barrier %arg38, 1, %true_0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc257)
      %29 = tt.call @"triton.experimental.gluon.language._standard.full_like__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32__(2,)cNone_(3,)cNone_(4,)cNone"(%result, %arg0) : (tensor<128x128xf32, #blocked3>, f32) -> tensor<128x128xf32, #blocked3> loc(#loc258)
      %30 = tt.call @__main__._mul_f32x2__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(%result, %29) : (tensor<128x128xf32, #blocked3>, tensor<128x128xf32, #blocked3>) -> tensor<128x128xf32, #blocked3> loc(#loc259)
      %31 = tt.expand_dims %23 {axis = 1 : i32} : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<128x1xf32, #blocked3> loc(#loc260)
      %cst = arith.constant 0.000000e+00 : f32 loc(#loc261)
      %cst_1 = arith.constant dense<0.000000e+00> : tensor<128x1xf32, #blocked3> loc(#loc261)
      %32 = arith.subf %cst_1, %31 : tensor<128x1xf32, #blocked3> loc(#loc261)
      %33 = tt.call @__main__._add_f32x2__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL_fp32S128_1SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(%30, %32) : (tensor<128x128xf32, #blocked3>, tensor<128x1xf32, #blocked3>) -> tensor<128x128xf32, #blocked3> loc(#loc262)
      %34 = tt.reshape %33 : tensor<128x128xf32, #blocked3> -> tensor<128x2x64xf32, #blocked4> loc(#loc263)
      %35 = tt.trans %34 {order = array<i32: 0, 2, 1>} : tensor<128x2x64xf32, #blocked4> -> tensor<128x64x2xf32, #blocked5> loc(#loc264)
      %outLHS, %outRHS = tt.split %35 : tensor<128x64x2xf32, #blocked5> -> tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc265)
      %36 = tt.call @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %19#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc266)
      %37 = math.exp2 %outLHS : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc267)
      %38 = ttng.tmem_subslice %36 {N = 0 : i32} : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc268)
      %39 = tt.fp_to_fp %37, rounding = rtne : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> -> tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc269)
      %true_2 = arith.constant true loc(#loc270)
      ttng.tmem_store %39, %38, %true_2 : tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc270)
      %40 = math.exp2 %outRHS : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc271)
      %41 = ttng.tmem_subslice %36 {N = 64 : i32} : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc272)
      %42 = tt.fp_to_fp %40, rounding = rtne : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> -> tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> loc(#loc273)
      %true_3 = arith.constant true loc(#loc274)
      ttng.tmem_store %42, %41, %true_3 : tensor<128x64xf8E5M2, #ttg.slice<{dim = 2, parent = #blocked5}>> -> !ttg.memdesc<128x64xf8E5M2, #tmem6, #ttng.tensor_memory, mutable, 128x128> loc(#loc274)
      %true_4 = arith.constant true loc(#loc275)
      ttng.arrive_barrier %19#1, 1, %true_4 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc275)
      %43:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg33, %arg34, %arg35, %arg36, %arg37) : (!ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<1xi8, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc276)
      %44 = tt.join %37, %40 : tensor<128x64xf32, #ttg.slice<{dim = 2, parent = #blocked5}>> -> tensor<128x64x2xf32, #blocked5> loc(#loc277)
      %45 = tt.trans %44 {order = array<i32: 0, 2, 1>} : tensor<128x64x2xf32, #blocked5> -> tensor<128x2x64xf32, #blocked4> loc(#loc278)
      %46 = tt.reshape %45 : tensor<128x2x64xf32, #blocked4> -> tensor<128x128xf32, #blocked3> loc(#loc279)
      %47 = ttg.convert_layout %46 : tensor<128x128xf32, #blocked3> -> tensor<128x128xf32, #blocked3> loc(#loc280)
      %48 = tt.call @"triton.language.standard.sum__fp32S128_128SLB1_128B32_1B4_1B0_1B1_1B1_1B1_0BL__(1,)cconstexpr_1__(2,)cconstexpr_False__(3,)cNone"(%47) : (tensor<128x128xf32, #blocked3>) -> tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc281)
      %49 = arith.mulf %arg40, %25 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc282)
      %50 = arith.addf %49, %48 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc283)
      scf.yield %19#2, %19#3, %19#4, %19#5, %19#6, %43#2, %43#3, %43#4, %43#5, %43#6, %43#1, %23, %50 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc284)
    } loc(#loc245)
    tt.return %5#11, %5#12, %5#10, %5#0, %5#1, %5#2, %5#3, %5#4, %5#5, %5#6, %5#7, %5#8, %5#9 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc285)
  ^bb1:  // no predecessors
    %6 = ub.poison : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc286)
    %7 = ub.poison : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> loc(#loc286)
    %8 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc286)
    %9 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc286)
    %10 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %11 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %12 = ub.poison : i32 loc(#loc286)
    %13 = ub.poison : i32 loc(#loc286)
    %14 = ub.poison : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc286)
    %15 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %16 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc286)
    %17 = ub.poison : i32 loc(#loc286)
    %18 = ub.poison : i32 loc(#loc286)
    tt.return %6, %7, %8, %9, %10, %11, %12, %13, %14, %15, %16, %17, %18 : tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, tensor<128xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x1xi8, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc286)
  } loc(#loc243)
  tt.func private @"__main__._attn_fwd_mma____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg4: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg5: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg6: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg7: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg8: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg9: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg13: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg14: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg15: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg16: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg17: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg19: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg20: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg21: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg22: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg23: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg24: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg25: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg28: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg29: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg30: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg31: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg32: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg33: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg34: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg35: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg36: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg37: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg38: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg39: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg40: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg41: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg42: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg43: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg44: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg45: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg46: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg47: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0), %arg48: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":536:0)) attributes {noinline = false} {
    %0:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg4, %arg5, %arg6) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc324)
    %1:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>__"(%arg7, %arg8, %arg9) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc325)
    %2:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg10, %arg11, %arg12) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc326)
    %3:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg16, %arg17, %arg18) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc327)
    %4:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg19, %arg20, %arg21) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc328)
    %5:8 = tt.call @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0, %arg1, %arg2, %arg3) : (f32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc329)
    %c78_i32 = arith.constant 78 : i32 loc(#loc330)
    %6 = arith.bitcast %5#4 : i32 to i32 loc(#loc330)
    %7 = arith.bitcast %5#7 : i32 to i32 loc(#loc330)
    %8 = arith.bitcast %c78_i32 : i32 to i32 loc(#loc330)
    %9 = ub.poison : i32 loc(#loc330)
    %10:25 = scf.for %arg49 = %6 to %7 step %8 iter_args(%arg50 = %0#0, %arg51 = %0#1, %arg52 = %0#2, %arg53 = %0#3, %arg54 = %0#4, %arg55 = %1#0, %arg56 = %1#1, %arg57 = %1#2, %arg58 = %1#3, %arg59 = %1#4, %arg60 = %2#0, %arg61 = %2#1, %arg62 = %2#2, %arg63 = %2#3, %arg64 = %2#4, %arg65 = %3#0, %arg66 = %3#1, %arg67 = %3#2, %arg68 = %3#3, %arg69 = %3#4, %arg70 = %4#0, %arg71 = %4#1, %arg72 = %4#2, %arg73 = %4#3, %arg74 = %4#4) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32)  : i32 {
      %11:8 = tt.call @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%5#0, %5#1, %5#2, %5#3, %5#4, %5#5, %5#6, %5#7, %arg49) : (f32, i32, i32, i32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc331)
      %12:2 = tt.call @"__main__.AttentionProgram.get_fused_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_1_"(%11#0, %11#1, %11#2, %11#3, %11#4, %11#5, %11#6, %11#7) : (f32, i32, i32, i32, i32, i32, i32, i32) -> (i32, i32) loc(#loc332)
      %13 = arith.extsi %12#1 : i32 to i64 loc(#loc333)
      %14 = arith.extsi %12#0 : i32 to i64 loc(#loc333)
      %15 = arith.subi %13, %14 : i64 loc(#loc333)
      %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc333)
      %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc333)
      %16 = arith.cmpi sle, %15, %c2147483647_i64 : i64 loc(#loc333)
      %17 = arith.cmpi sge, %15, %c-2147483648_i64 : i64 loc(#loc333)
      %18 = arith.andi %16, %17 : i1 loc(#loc333)
      %19 = arith.subi %12#1, %12#0 : i32 loc(#loc333)
      %c128_i32 = arith.constant 128 : i32 loc(#loc334)
      %c128_i32_0 = arith.constant 128 : i32 loc(#loc334)
      %20 = arith.divsi %19, %c128_i32_0 : i32 loc(#loc334)
      %21:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg50, %arg51, %arg52, %arg53, %arg54) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc335)
      %22:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%arg55, %arg56, %arg57, %arg58, %arg59) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc336)
      %23:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg65, %arg66, %arg67, %arg68, %arg69) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc337)
      %24 = ttg.memdesc_trans %22#0 {order = array<i32: 1, 0>} : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> -> !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128> loc(#loc338)
      tt.call @"__main__.tcgen05_mma__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDfp8e5S64_128SLNVMMA_64_8_True_False_NVMMALAS[8, 64, 128]ASMD_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD__(3,)cconstexpr_False_"(%21#0, %24, %23#0, %23#1) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) -> () loc(#loc339)
      %25:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%21#2, %21#3, %21#4, %21#5, %21#6) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc340)
      %26:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg70, %arg71, %arg72, %arg73, %arg74) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc341)
      %27 = ttg.memdesc_trans %22#0 {order = array<i32: 1, 0>} : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> -> !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128> loc(#loc342)
      tt.call @"__main__.tcgen05_mma__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDfp8e5S64_128SLNVMMA_64_8_True_False_NVMMALAS[8, 64, 128]ASMD_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD__(3,)cconstexpr_False_"(%25#0, %27, %26#0, %26#1, %22#1) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) -> () loc(#loc343)
      %28:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%22#2, %22#3, %22#4, %22#5, %22#6) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc344)
      %29:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg60, %arg61, %arg62, %arg63, %arg64) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc345)
      %30:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%23#2, %23#3, %23#4, %23#5, %23#6) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc346)
      %31 = tt.call @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %30#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc347)
      tt.call @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD__(3,)cconstexpr_False_"(%31, %28#0, %29#0, %29#1) : (!ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) -> () loc(#loc348)
      %false = arith.constant false loc(#loc349)
      %c1_i32 = arith.constant 1 : i32 loc(#loc350)
      %c1_i32_1 = arith.constant 1 : i32 loc(#loc350)
      %32 = arith.extsi %20 : i32 to i64 loc(#loc350)
      %33 = arith.extsi %c1_i32_1 : i32 to i64 loc(#loc350)
      %34 = arith.subi %32, %33 : i64 loc(#loc350)
      %c2147483647_i64_2 = arith.constant 2147483647 : i64 loc(#loc350)
      %c-2147483648_i64_3 = arith.constant -2147483648 : i64 loc(#loc350)
      %35 = arith.cmpi sle, %34, %c2147483647_i64_2 : i64 loc(#loc350)
      %36 = arith.cmpi sge, %34, %c-2147483648_i64_3 : i64 loc(#loc350)
      %37 = arith.andi %35, %36 : i1 loc(#loc350)
      %38 = arith.subi %20, %c1_i32_1 : i32 loc(#loc350)
      %c1_i32_4 = arith.constant 1 : i32 loc(#loc350)
      %c1_i32_5 = arith.constant 1 : i32 loc(#loc350)
      %39 = arith.extsi %20 : i32 to i64 loc(#loc350)
      %40 = arith.extsi %c1_i32_5 : i32 to i64 loc(#loc350)
      %41 = arith.subi %39, %40 : i64 loc(#loc350)
      %c2147483647_i64_6 = arith.constant 2147483647 : i64 loc(#loc350)
      %c-2147483648_i64_7 = arith.constant -2147483648 : i64 loc(#loc350)
      %42 = arith.cmpi sle, %41, %c2147483647_i64_6 : i64 loc(#loc350)
      %43 = arith.cmpi sge, %41, %c-2147483648_i64_7 : i64 loc(#loc350)
      %44 = arith.andi %42, %43 : i1 loc(#loc350)
      %45 = arith.subi %20, %c1_i32_5 : i32 loc(#loc350)
      %c0_i32 = arith.constant 0 : i32 loc(#loc351)
      %c1_i32_8 = arith.constant 1 : i32 loc(#loc351)
      %46 = arith.bitcast %c0_i32 : i32 to i32 loc(#loc351)
      %47 = arith.bitcast %45 : i32 to i32 loc(#loc351)
      %48 = arith.bitcast %c1_i32_8 : i32 to i32 loc(#loc351)
      %49 = ub.poison : i32 loc(#loc351)
      %50:32 = scf.for %arg75 = %46 to %47 step %48 iter_args(%arg76 = %28#2, %arg77 = %28#3, %arg78 = %28#4, %arg79 = %28#5, %arg80 = %28#6, %arg81 = %29#2, %arg82 = %29#3, %arg83 = %29#4, %arg84 = %29#5, %arg85 = %29#6, %arg86 = %30#2, %arg87 = %30#3, %arg88 = %30#4, %arg89 = %30#5, %arg90 = %30#6, %arg91 = %26#2, %arg92 = %26#3, %arg93 = %26#4, %arg94 = %26#5, %arg95 = %26#6, %arg96 = %22#0, %arg97 = %22#1, %arg98 = %30#0, %arg99 = %30#1, %arg100 = %26#0, %arg101 = %26#1, %arg102 = %28#0, %arg103 = %28#1, %arg104 = %29#0, %arg105 = %29#1, %arg106 = %31, %arg107 = %false) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, i1)  : i32 {
        %54:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%arg76, %arg77, %arg78, %arg79, %arg80) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc352)
        %55 = ttg.memdesc_trans %54#0 {order = array<i32: 1, 0>} : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> -> !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128> loc(#loc353)
        tt.call @"__main__.tcgen05_mma__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDfp8e5S64_128SLNVMMA_64_8_True_False_NVMMALAS[8, 64, 128]ASMD_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD__(3,)cconstexpr_False_"(%21#0, %55, %arg98, %arg99) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) -> () loc(#loc354)
        %56:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg81, %arg82, %arg83, %arg84, %arg85) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc355)
        %57:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg91, %arg92, %arg93, %arg94, %arg95) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc356)
        %58 = tt.call @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %57#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc357)
        tt.call @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_u1_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD__"(%58, %arg102, %56#0, %arg107, %56#1, %arg103) : (!ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, i1, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) -> () loc(#loc358)
        %true = arith.constant true loc(#loc359)
        %59 = ttg.memdesc_trans %54#0 {order = array<i32: 1, 0>} : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> -> !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128> loc(#loc360)
        tt.call @"__main__.tcgen05_mma__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDfp8e5S64_128SLNVMMA_64_8_True_False_NVMMALAS[8, 64, 128]ASMD_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD__(3,)cconstexpr_False_"(%25#0, %59, %57#0, %57#1, %54#1) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) -> () loc(#loc361)
        %60:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%54#2, %54#3, %54#4, %54#5, %54#6) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc362)
        %61:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%56#2, %56#3, %56#4, %56#5, %56#6) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc363)
        %62:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg86, %arg87, %arg88, %arg89, %arg90) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc364)
        %63 = tt.call @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %62#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc365)
        tt.call @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_u1_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD__"(%63, %60#0, %61#0, %true, %61#1) : (!ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, i1, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) -> () loc(#loc366)
        %true_9 = arith.constant true loc(#loc367)
        scf.yield %60#2, %60#3, %60#4, %60#5, %60#6, %61#2, %61#3, %61#4, %61#5, %61#6, %62#2, %62#3, %62#4, %62#5, %62#6, %57#2, %57#3, %57#4, %57#5, %57#6, %54#0, %54#1, %62#0, %62#1, %57#0, %57#1, %60#0, %60#1, %61#0, %61#1, %63, %true_9 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, i1 loc(#loc368)
      } loc(#loc351)
      ttng.tc_gen5_commit %21#1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc369)
      ttng.tc_gen5_commit %25#1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc370)
      %51:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%50#5, %50#6, %50#7, %50#8, %50#9) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc371)
      %52:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%50#15, %50#16, %50#17, %50#18, %50#19) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc372)
      %53 = tt.call @"__main__._borrow_s_as_p____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD__"(%arg0, %arg1, %arg2, %arg3, %52#0) : (f32, i32, i32, i32, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>) -> !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc(#loc373)
      tt.call @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_u1_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD__"(%53, %50#26, %51#0, %50#31, %51#1, %50#27, %50#23, %52#1) : (!ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, i1, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) -> () loc(#loc374)
      scf.yield %25#2, %25#3, %25#4, %25#5, %25#6, %50#0, %50#1, %50#2, %50#3, %50#4, %51#2, %51#3, %51#4, %51#5, %51#6, %50#10, %50#11, %50#12, %50#13, %50#14, %52#2, %52#3, %52#4, %52#5, %52#6 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc375)
    } loc(#loc330)
    tt.return loc(#loc376)
  } loc(#loc323)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0)) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc62)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc63)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc64)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc65)
    %1 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %2 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %3 = ub.poison : i32 loc(#loc65)
    %4 = ub.poison : i32 loc(#loc65)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc65)
  } loc(#loc61)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":175:0)) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc62)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc63)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc64)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc65)
    %1 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %2 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc65)
    %3 = ub.poison : i32 loc(#loc65)
    %4 = ub.poison : i32 loc(#loc65)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc65)
  } loc(#loc61)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0)) -> (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc67)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc68)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc69)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc70)
    %1 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %2 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %3 = ub.poison : i32 loc(#loc70)
    %4 = ub.poison : i32 loc(#loc70)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc70)
  } loc(#loc66)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0)) -> (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc67)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc68)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc69)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc70)
    %1 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %2 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %3 = ub.poison : i32 loc(#loc70)
    %4 = ub.poison : i32 loc(#loc70)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc70)
  } loc(#loc66)
  tt.func private @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) loc(#loc112)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc113)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc114)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc115)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc115)
    %4 = ub.poison : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc115)
    %5 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %6 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %7 = ub.poison : i32 loc(#loc115)
    %8 = ub.poison : i32 loc(#loc115)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc115)
  } loc(#loc111)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc117)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc117)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc118)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc118)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc119)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc119)
    %true = arith.constant true loc(#loc120)
    ttng.wait_barrier %1, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc120)
    tt.return %0, %2 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc121)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc122)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc122)
    tt.return %3, %4 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc122)
  } loc(#loc116)
  tt.func private @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":215:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) loc(#loc112)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc113)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc114)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc115)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc115)
    %4 = ub.poison : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc115)
    %5 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %6 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc115)
    %7 = ub.poison : i32 loc(#loc115)
    %8 = ub.poison : i32 loc(#loc115)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc115)
  } loc(#loc111)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":162:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc117)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc117)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc118)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc118)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc119)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc119)
    %true = arith.constant true loc(#loc120)
    ttng.wait_barrier %1, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc120)
    tt.return %0, %2 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc121)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc122)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc122)
    tt.return %3, %4 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc122)
  } loc(#loc116)
  tt.func private @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":143:0)) -> (i32, i32) attributes {noinline = false} {
    %c1_i32 = arith.constant 1 : i32 loc(#loc127)
    %c1_i32_0 = arith.constant 1 : i32 loc(#loc127)
    %0 = arith.extsi %arg3 : i32 to i64 loc(#loc127)
    %1 = arith.extsi %c1_i32_0 : i32 to i64 loc(#loc127)
    %2 = arith.addi %0, %1 : i64 loc(#loc127)
    %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc127)
    %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc127)
    %3 = arith.cmpi sle, %2, %c2147483647_i64 : i64 loc(#loc127)
    %4 = arith.cmpi sge, %2, %c-2147483648_i64 : i64 loc(#loc127)
    %5 = arith.andi %3, %4 : i1 loc(#loc127)
    %6 = arith.addi %arg3, %c1_i32_0 : i32 loc(#loc127)
    %c8_i32 = arith.constant 8 : i32 loc(#loc128)
    %7 = arith.cmpi eq, %6, %c8_i32 : i32 loc(#loc128)
    %c0_i32 = arith.constant 0 : i32 loc(#loc129)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc129)
    %8 = arith.select %7, %c0_i32_1, %6 : i32 loc(#loc129)
    %c1_i32_2 = arith.constant 1 : i32 loc(#loc130)
    %c1_i32_3 = arith.constant 1 : i32 loc(#loc130)
    %9 = arith.xori %arg4, %c1_i32_3 : i32 loc(#loc130)
    %10 = arith.select %7, %9, %arg4 : i32 loc(#loc131)
    tt.return %8, %10 : i32, i32 loc(#loc132)
  ^bb1:  // no predecessors
    %11 = ub.poison : i32 loc(#loc133)
    %12 = ub.poison : i32 loc(#loc133)
    tt.return %11, %12 : i32, i32 loc(#loc133)
  } loc(#loc123)
  tt.func private @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0)) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) loc(#loc202)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc203)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc204)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc205)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc205)
    %4 = ub.poison : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc(#loc205)
    %5 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %6 = ub.poison : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %7 = ub.poison : i32 loc(#loc205)
    %8 = ub.poison : i32 loc(#loc205)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<1x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc205)
  } loc(#loc201)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0)) -> (!ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc207)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc207)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc208)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc208)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc209)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc209)
    %true = arith.constant true loc(#loc210)
    ttng.wait_barrier %2, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc210)
    tt.return %0, %1 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc211)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc(#loc212)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc212)
    tt.return %3, %4 : !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc212)
  } loc(#loc206)
  tt.func private @"__main__.tcgen05_mma__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDfp8e5S64_128SLNVMMA_64_8_True_False_NVMMALAS[8, 64, 128]ASMD_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD__(3,)cconstexpr_False_"(%arg0: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg1: !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg2: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg3: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)) attributes {noinline = false} {
    %false = arith.constant false loc(#loc378)
    %true = arith.constant true loc(#loc378)
    %true_0 = arith.constant true loc(#loc378)
    %0 = ttng.tc_gen5_mma %arg0, %arg1, %arg2[], %false, %true, %arg3[%true_0] : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc378)
    tt.return loc(#loc379)
  } loc(#loc377)
  tt.func private @"__main__.tcgen05_mma__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDfp8e5S64_128SLNVMMA_64_8_True_False_NVMMALAS[8, 64, 128]ASMD_MDfp32S128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD__(3,)cconstexpr_False_"(%arg0: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg1: !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg2: !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg3: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg4: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)) attributes {noinline = false} {
    %false = arith.constant false loc(#loc378)
    %true = arith.constant true loc(#loc378)
    %true_0 = arith.constant true loc(#loc378)
    %true_1 = arith.constant true loc(#loc378)
    %0 = ttng.tc_gen5_mma %arg0, %arg1, %arg2[], %false, %true, %arg3[%true_0], %arg4[%true_1] : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<64x128xf8E5M2, #shared2, #smem, mutable, 8x64x128>, !ttg.memdesc<128x128xf32, #tmem1, #ttng.tensor_memory, mutable, 1x128x128>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc378)
    tt.return loc(#loc379)
  } loc(#loc377)
  tt.func private @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0)) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) loc(#loc202)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc203)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc204)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc205)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc205)
    %4 = ub.poison : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc(#loc205)
    %5 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %6 = ub.poison : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %7 = ub.poison : i32 loc(#loc205)
    %8 = ub.poison : i32 loc(#loc205)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc205)
  } loc(#loc201)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0)) -> (!ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc207)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> -> !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc207)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc208)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc208)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc209)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc209)
    %true = arith.constant true loc(#loc210)
    ttng.wait_barrier %2, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc210)
    tt.return %0, %1 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc211)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc(#loc212)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc212)
    tt.return %3, %4 : !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc212)
  } loc(#loc206)
  tt.func private @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD__(3,)cconstexpr_False_"(%arg0: !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg1: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg2: !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg3: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)) attributes {noinline = false} {
    %false = arith.constant false loc(#loc378)
    %true = arith.constant true loc(#loc378)
    %true_0 = arith.constant true loc(#loc378)
    %0 = ttng.tc_gen5_mma %arg0, %arg1, %arg2[], %false, %true, %arg3[%true_0] : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc378)
    tt.return loc(#loc379)
  } loc(#loc377)
  tt.func private @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_u1_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD__"(%arg0: !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg1: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg2: !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg3: i1 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg4: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg5: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)) attributes {noinline = false} {
    %true = arith.constant true loc(#loc378)
    %true_0 = arith.constant true loc(#loc378)
    %true_1 = arith.constant true loc(#loc378)
    %0 = ttng.tc_gen5_mma %arg0, %arg1, %arg2[], %arg3, %true, %arg4[%true_0], %arg5[%true_1] : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc378)
    tt.return loc(#loc379)
  } loc(#loc377)
  tt.func private @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_u1_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD__"(%arg0: !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg1: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg2: !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg3: i1 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg4: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)) attributes {noinline = false} {
    %true = arith.constant true loc(#loc378)
    %true_0 = arith.constant true loc(#loc378)
    %0 = ttng.tc_gen5_mma %arg0, %arg1, %arg2[], %arg3, %true, %arg4[%true_0] : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc378)
    tt.return loc(#loc379)
  } loc(#loc377)
  tt.func private @"__main__.tcgen05_mma__MDfp8e5S128_128SLTL128xconstexpr[128]PTLLAS[128, 128]ASMD_MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDfp32S128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD_u1_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD__"(%arg0: !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg1: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg2: !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg3: i1 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg4: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg5: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg6: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0), %arg7: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":252:0)) attributes {noinline = false} {
    %true = arith.constant true loc(#loc378)
    %true_0 = arith.constant true loc(#loc378)
    %true_1 = arith.constant true loc(#loc378)
    %true_2 = arith.constant true loc(#loc378)
    %true_3 = arith.constant true loc(#loc378)
    %0 = ttng.tc_gen5_mma %arg0, %arg1, %arg2[], %arg3, %true, %arg4[%true_0], %arg5[%true_1], %arg6[%true_2], %arg7[%true_3] : !ttg.memdesc<128x128xf8E5M2, #tmem5, #ttng.tensor_memory, mutable>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<128x64xf32, #tmem, #ttng.tensor_memory, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc378)
    tt.return loc(#loc379)
  } loc(#loc377)
  tt.func private @"__main__._attn_fwd_load____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg4: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg5: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg6: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg7: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg8: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg9: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg13: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg14: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg15: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg16: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg17: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg19: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg20: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg21: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg22: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg23: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg24: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg25: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg28: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg29: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg30: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg31: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg32: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg33: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg34: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg35: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg36: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg37: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg38: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg39: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg40: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg41: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg42: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg43: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg44: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg45: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg46: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg47: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0), %arg48: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":500:0)) attributes {noinline = false} {
    %0:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg4, %arg5, %arg6) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc381)
    %1:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>__"(%arg7, %arg8, %arg9) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc382)
    %2:8 = tt.call @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0, %arg1, %arg2, %arg3) : (f32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc383)
    %c78_i32 = arith.constant 78 : i32 loc(#loc384)
    %3 = arith.bitcast %2#4 : i32 to i32 loc(#loc384)
    %4 = arith.bitcast %2#7 : i32 to i32 loc(#loc384)
    %5 = arith.bitcast %c78_i32 : i32 to i32 loc(#loc384)
    %6 = ub.poison : i32 loc(#loc384)
    %7:10 = scf.for %arg49 = %3 to %4 step %5 iter_args(%arg50 = %0#0, %arg51 = %0#1, %arg52 = %0#2, %arg53 = %0#3, %arg54 = %0#4, %arg55 = %1#0, %arg56 = %1#1, %arg57 = %1#2, %arg58 = %1#3, %arg59 = %1#4) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32)  : i32 {
      %8:8 = tt.call @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%2#0, %2#1, %2#2, %2#3, %2#4, %2#5, %2#6, %2#7, %arg49) : (f32, i32, i32, i32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc385)
      %9:2 = tt.call @"__main__.AttentionProgram.get_fused_loop_bounds____main__.AttentionProgram<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>__(1,)cconstexpr_1_"(%8#0, %8#1, %8#2, %8#3, %8#4, %8#5, %8#6, %8#7) : (f32, i32, i32, i32, i32, i32, i32, i32) -> (i32, i32) loc(#loc386)
      %c0_i32 = arith.constant 0 : i32 loc(#loc387)
      %c0_i32_0 = arith.constant 0 : i32 loc(#loc387)
      %10 = arith.extsi %8#7 : i32 to i64 loc(#loc387)
      %11 = arith.extsi %c0_i32_0 : i32 to i64 loc(#loc387)
      %12 = arith.addi %10, %11 : i64 loc(#loc387)
      %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc387)
      %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc387)
      %13 = arith.cmpi sle, %12, %c2147483647_i64 : i64 loc(#loc387)
      %14 = arith.cmpi sge, %12, %c-2147483648_i64 : i64 loc(#loc387)
      %15 = arith.andi %13, %14 : i1 loc(#loc387)
      %16 = arith.addi %8#7, %c0_i32_0 : i32 loc(#loc387)
      %17:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg50, %arg51, %arg52, %arg53, %arg54) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc388)
      tt.call @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%17#0, %17#1, %arg28, %arg29, %arg30, %arg31, %arg32, %16) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, i32) -> () loc(#loc389)
      %18 = arith.extsi %8#6 : i32 to i64 loc(#loc390)
      %19 = arith.extsi %9#0 : i32 to i64 loc(#loc390)
      %20 = arith.addi %18, %19 : i64 loc(#loc390)
      %c2147483647_i64_1 = arith.constant 2147483647 : i64 loc(#loc390)
      %c-2147483648_i64_2 = arith.constant -2147483648 : i64 loc(#loc390)
      %21 = arith.cmpi sle, %20, %c2147483647_i64_1 : i64 loc(#loc390)
      %22 = arith.cmpi sge, %20, %c-2147483648_i64_2 : i64 loc(#loc390)
      %23 = arith.andi %21, %22 : i1 loc(#loc390)
      %24 = arith.addi %8#6, %9#0 : i32 loc(#loc390)
      %25:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%arg55, %arg56, %arg57, %arg58, %arg59) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc391)
      tt.call @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%25#0, %25#1, %arg33, %arg34, %arg35, %arg36, %arg37, %24) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, i32) -> () loc(#loc392)
      %c128_i32 = arith.constant 128 : i32 loc(#loc393)
      %c128_i32_3 = arith.constant 128 : i32 loc(#loc393)
      %26 = arith.extsi %8#7 : i32 to i64 loc(#loc393)
      %27 = arith.extsi %c128_i32_3 : i32 to i64 loc(#loc393)
      %28 = arith.addi %26, %27 : i64 loc(#loc393)
      %c2147483647_i64_4 = arith.constant 2147483647 : i64 loc(#loc393)
      %c-2147483648_i64_5 = arith.constant -2147483648 : i64 loc(#loc393)
      %29 = arith.cmpi sle, %28, %c2147483647_i64_4 : i64 loc(#loc393)
      %30 = arith.cmpi sge, %28, %c-2147483648_i64_5 : i64 loc(#loc393)
      %31 = arith.andi %29, %30 : i1 loc(#loc393)
      %32 = arith.addi %8#7, %c128_i32_3 : i32 loc(#loc393)
      %33:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%17#2, %17#3, %17#4, %17#5, %17#6) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc394)
      tt.call @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%33#0, %33#1, %arg28, %arg29, %arg30, %arg31, %arg32, %32) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, i32) -> () loc(#loc395)
      %34:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%25#2, %25#3, %25#4, %25#5, %25#6) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc396)
      tt.call @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%34#0, %34#1, %arg38, %arg39, %arg40, %arg41, %arg42, %24) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, i32) -> () loc(#loc397)
      %c128_i32_6 = arith.constant 128 : i32 loc(#loc398)
      %c128_i32_7 = arith.constant 128 : i32 loc(#loc398)
      %35 = arith.extsi %9#0 : i32 to i64 loc(#loc398)
      %36 = arith.extsi %c128_i32_7 : i32 to i64 loc(#loc398)
      %37 = arith.addi %35, %36 : i64 loc(#loc398)
      %c2147483647_i64_8 = arith.constant 2147483647 : i64 loc(#loc398)
      %c-2147483648_i64_9 = arith.constant -2147483648 : i64 loc(#loc398)
      %38 = arith.cmpi sle, %37, %c2147483647_i64_8 : i64 loc(#loc398)
      %39 = arith.cmpi sge, %37, %c-2147483648_i64_9 : i64 loc(#loc398)
      %40 = arith.andi %38, %39 : i1 loc(#loc398)
      %41 = arith.addi %9#0, %c128_i32_7 : i32 loc(#loc398)
      %c128_i32_10 = arith.constant 128 : i32 loc(#loc399)
      %42 = arith.bitcast %41 : i32 to i32 loc(#loc399)
      %43 = arith.bitcast %9#1 : i32 to i32 loc(#loc399)
      %44 = arith.bitcast %c128_i32_10 : i32 to i32 loc(#loc399)
      %45 = ub.poison : i32 loc(#loc399)
      %46:10 = scf.for %arg60 = %42 to %43 step %44 iter_args(%arg61 = %34#2, %arg62 = %34#3, %arg63 = %34#4, %arg64 = %34#5, %arg65 = %34#6, %arg66 = %24, %arg67 = %25#0, %arg68 = %25#1, %arg69 = %34#0, %arg70 = %34#1) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32, i32, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>)  : i32 {
        %47 = arith.extsi %8#6 : i32 to i64 loc(#loc400)
        %48 = arith.extsi %arg60 : i32 to i64 loc(#loc400)
        %49 = arith.addi %47, %48 : i64 loc(#loc400)
        %c2147483647_i64_11 = arith.constant 2147483647 : i64 loc(#loc400)
        %c-2147483648_i64_12 = arith.constant -2147483648 : i64 loc(#loc400)
        %50 = arith.cmpi sle, %49, %c2147483647_i64_11 : i64 loc(#loc400)
        %51 = arith.cmpi sge, %49, %c-2147483648_i64_12 : i64 loc(#loc400)
        %52 = arith.andi %50, %51 : i1 loc(#loc400)
        %53 = arith.addi %8#6, %arg60 : i32 loc(#loc400)
        %54:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%arg61, %arg62, %arg63, %arg64, %arg65) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc401)
        tt.call @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%54#0, %54#1, %arg33, %arg34, %arg35, %arg36, %arg37, %53) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, i32) -> () loc(#loc402)
        %55:7 = tt.call @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%54#2, %54#3, %54#4, %54#5, %54#6) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc403)
        tt.call @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%55#0, %55#1, %arg38, %arg39, %arg40, %arg41, %arg42, %53) : (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, i32, i32, i64, i64, i32) -> () loc(#loc404)
        scf.yield %55#2, %55#3, %55#4, %55#5, %55#6, %53, %54#0, %54#1, %55#0, %55#1 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32, i32, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc405)
      } loc(#loc399)
      scf.yield %33#2, %33#3, %33#4, %33#5, %33#6, %46#0, %46#1, %46#2, %46#3, %46#4 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc406)
    } loc(#loc384)
    tt.return loc(#loc407)
  } loc(#loc380)
  tt.func private @"__main__.Channel.<locals>.ChannelType.create_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":171:0)) -> (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc67)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc68)
    tt.return %arg0, %arg1, %arg2, %c0_i32, %c0_i32_0 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc69)
  ^bb1:  // no predecessors
    %0 = ub.poison : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc70)
    %1 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %2 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc70)
    %3 = ub.poison : i32 loc(#loc70)
    %4 = ub.poison : i32 loc(#loc70)
    tt.return %0, %1, %2, %3, %4 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc70)
  } loc(#loc66)
  tt.func private @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%arg0: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg1: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg2: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg5: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg6: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0)) attributes {noinline = false} {
    %true = arith.constant true loc(#loc409)
    ttng.barrier_expect %arg1, 8192, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc409)
    %c0_i32 = arith.constant 0 : i32 loc(#loc410)
    %true_0 = arith.constant true loc(#loc410)
    ttng.async_tma_copy_global_to_local %arg2[%arg7, %c0_i32] %arg0, %arg1, %true_0 : !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc410)
    tt.return loc(#loc411)
  } loc(#loc408)
  tt.func private @"__main__.Channel.<locals>.Producer.acquire____main__.Channel.<locals>.Producer<__main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>, i32, i32>__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":198:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) attributes {noinline = false} {
    %0:2 = tt.call @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) loc(#loc202)
    %1:2 = tt.call @"__main__.Channel.<locals>.ChannelType.increment____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0, %arg1, %arg2, %arg4, %arg3) : (!ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32) -> (i32, i32) loc(#loc203)
    tt.return %0#0, %0#1, %arg0, %arg1, %arg2, %1#1, %1#0 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc204)
  ^bb1:  // no predecessors
    %2 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc205)
    %3 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc205)
    %4 = ub.poison : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc205)
    %5 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %6 = ub.poison : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc(#loc205)
    %7 = ub.poison : i32 loc(#loc205)
    %8 = ub.poison : i32 loc(#loc205)
    tt.return %2, %3, %4, %5, %6, %7, %8 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>, !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<8x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc205)
  } loc(#loc201)
  tt.func private @"__main__.Channel.<locals>.ChannelType.acquire_producer____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>_i32_i32__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":153:0)) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1>) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc207)
    %0 = ttg.memdesc_subview %arg0[%arg3, %c0_i32, %c0_i32] : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc207)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc208)
    %1 = ttg.memdesc_subview %arg1[%arg3, %c0_i32_0] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc208)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc209)
    %2 = ttg.memdesc_subview %arg2[%arg3, %c0_i32_1] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc209)
    %true = arith.constant true loc(#loc210)
    ttng.wait_barrier %2, %arg4, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc210)
    tt.return %0, %1 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc211)
  ^bb1:  // no predecessors
    %3 = ub.poison : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc212)
    %4 = ub.poison : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc212)
    tt.return %3, %4 : !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc212)
  } loc(#loc206)
  tt.func private @"__main__.issue_async_tma_load__MDfp8e5S128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD_MDi64S1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_i32__"(%arg0: !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg1: !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg2: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg4: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg5: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg6: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0), %arg7: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":245:0)) attributes {noinline = false} {
    %true = arith.constant true loc(#loc409)
    ttng.barrier_expect %arg1, 8192, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc409)
    %c0_i32 = arith.constant 0 : i32 loc(#loc410)
    %true_0 = arith.constant true loc(#loc410)
    ttng.async_tma_copy_global_to_local %arg2[%arg7, %c0_i32] %arg0, %arg1, %true_0 : !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> -> !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 8x128x64> loc(#loc410)
    tt.return loc(#loc411)
  } loc(#loc408)
  tt.func private @"__main__._attn_fwd_epilogue____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>___main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_TD<bound method block_type.mangle of <['128', '64'], fp8e5>>_NVMMA_64_8_False_False_NVMMATD_Pfp32__(4,)cconstexpr_1_"(%arg0: f32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg1: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg2: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg3: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg4: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg5: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg6: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg7: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg8: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg9: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg10: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg11: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg12: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg13: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg14: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg15: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg16: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg17: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg18: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg19: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg20: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg21: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg22: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg23: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg24: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg25: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg26: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg27: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg28: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg29: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg30: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg31: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg32: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg33: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg34: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg35: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg36: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg37: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg38: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg39: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg40: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg41: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg42: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg43: !tt.tensordesc<tensor<128x64xf8E5M2, #shared>> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg44: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg45: i32 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg46: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg47: i64 loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0), %arg48: !tt.ptr<f32> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":699:0)) attributes {noinline = false} {
    %0:5 = tt.call @"__main__.Channel.<locals>.ChannelType.create_consumer____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg13, %arg14, %arg15) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc413)
    %1:8 = tt.call @"__main__.ProgramScheduler.create____main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>__"(%arg0, %arg1, %arg2, %arg3) : (f32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc414)
    %c78_i32 = arith.constant 78 : i32 loc(#loc415)
    %2 = arith.bitcast %1#4 : i32 to i32 loc(#loc415)
    %3 = arith.bitcast %1#7 : i32 to i32 loc(#loc415)
    %4 = arith.bitcast %c78_i32 : i32 to i32 loc(#loc415)
    %5 = ub.poison : i32 loc(#loc415)
    %6:5 = scf.for %arg49 = %2 to %3 step %4 iter_args(%arg50 = %0#0, %arg51 = %0#1, %arg52 = %0#2, %arg53 = %0#3, %arg54 = %0#4) -> (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32)  : i32 {
      %7:8 = tt.call @"__main__.ProgramScheduler.get_program____main__.ProgramScheduler<__main__.AttentionConfig<fp32, i32, i32, i32, constexpr[constexpr[256]], constexpr[constexpr[128]], constexpr[constexpr[64]], constexpr[constexpr[8]], constexpr[constexpr[78]], constexpr[constexpr[fp8e5]], constexpr[constexpr[4]], constexpr[constexpr[2]], constexpr[constexpr[128]], constexpr[constexpr[32]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[[constexpr[128], constexpr[128]]], constexpr[[constexpr[128], constexpr[64]]], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[64]), unpacked=True, cta_split_num=None)], constexpr[TensorMemoryLayout(block=(128, constexpr[128]), unpacked=False, cta_split_num=None)], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 128], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 64], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[constexpr[BlockedLayout(size_per_thread=[1, 32], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])]], constexpr[BlockedLayout(size_per_thread=[1, 1], threads_per_warp=[32, 1], warps_per_cta=[4, 1], order=[0, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[1, 0])], constexpr[8]>, i32, i32, i32, i32>_i32__"(%1#0, %1#1, %1#2, %1#3, %1#4, %1#5, %1#6, %1#7, %arg49) : (f32, i32, i32, i32, i32, i32, i32, i32, i32) -> (f32, i32, i32, i32, i32, i32, i32, i32) loc(#loc416)
      %8:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%arg50, %arg51, %arg52, %arg53, %arg54) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc417)
      %c0_i32 = arith.constant 0 : i32 loc(#loc418)
      %c0_i32_0 = arith.constant 0 : i32 loc(#loc418)
      %9 = arith.extsi %7#7 : i32 to i64 loc(#loc418)
      %10 = arith.extsi %c0_i32_0 : i32 to i64 loc(#loc418)
      %11 = arith.addi %9, %10 : i64 loc(#loc418)
      %c2147483647_i64 = arith.constant 2147483647 : i64 loc(#loc418)
      %c-2147483648_i64 = arith.constant -2147483648 : i64 loc(#loc418)
      %12 = arith.cmpi sle, %11, %c2147483647_i64 : i64 loc(#loc418)
      %13 = arith.cmpi sge, %11, %c-2147483648_i64 : i64 loc(#loc418)
      %14 = arith.andi %12, %13 : i1 loc(#loc418)
      %15 = arith.addi %7#7, %c0_i32_0 : i32 loc(#loc418)
      %c0_i32_1 = arith.constant 0 : i32 loc(#loc419)
      ttng.async_tma_copy_local_to_global %arg43[%15, %c0_i32_1] %8#0 : !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc419)
      %16:7 = tt.call @"__main__.Channel.<locals>.Consumer.acquire____main__.Channel.<locals>.Consumer<__main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>, i32, i32>__"(%8#2, %8#3, %8#4, %8#5, %8#6) : (!ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) -> (!ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64>, !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1>, !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32) loc(#loc420)
      %c128_i32 = arith.constant 128 : i32 loc(#loc421)
      %c128_i32_2 = arith.constant 128 : i32 loc(#loc421)
      %17 = arith.extsi %7#7 : i32 to i64 loc(#loc421)
      %18 = arith.extsi %c128_i32_2 : i32 to i64 loc(#loc421)
      %19 = arith.addi %17, %18 : i64 loc(#loc421)
      %c2147483647_i64_3 = arith.constant 2147483647 : i64 loc(#loc421)
      %c-2147483648_i64_4 = arith.constant -2147483648 : i64 loc(#loc421)
      %20 = arith.cmpi sle, %19, %c2147483647_i64_3 : i64 loc(#loc421)
      %21 = arith.cmpi sge, %19, %c-2147483648_i64_4 : i64 loc(#loc421)
      %22 = arith.andi %20, %21 : i1 loc(#loc421)
      %23 = arith.addi %7#7, %c128_i32_2 : i32 loc(#loc421)
      %c0_i32_5 = arith.constant 0 : i32 loc(#loc422)
      ttng.async_tma_copy_local_to_global %arg43[%23, %c0_i32_5] %16#0 : !tt.tensordesc<tensor<128x64xf8E5M2, #shared>>, !ttg.memdesc<128x64xf8E5M2, #shared, #smem, mutable, 2x128x64> loc(#loc422)
      ttng.async_tma_store_wait {pendings = 1 : i32} loc(#loc423)
      %true = arith.constant true loc(#loc424)
      ttng.arrive_barrier %8#1, 1, %true : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc424)
      ttng.async_tma_store_wait {pendings = 0 : i32} loc(#loc425)
      %true_6 = arith.constant true loc(#loc426)
      ttng.arrive_barrier %16#1, 1, %true_6 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc426)
      scf.yield %16#2, %16#3, %16#4, %16#5, %16#6 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, !ttg.memdesc<2x1xi64, #shared1, #smem, mutable>, i32, i32 loc(#loc427)
    } loc(#loc415)
    tt.return loc(#loc428)
  } loc(#loc412)
  tt.func private @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp8e5S2_128_64SLNVMMA_64_8_False_False_NVMMALAS[2, 128, 64]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0)) attributes {noinline = false} {
    ttg.local_dealloc %arg0 : !ttg.memdesc<2x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc430)
    %c0_i32 = arith.constant 0 : i32 loc(#loc431)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc431)
    %0 = ttg.memdesc_subview %arg1[%c0_i32_0, %c0_i32] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc431)
    ttng.inval_barrier %0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc432)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc433)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc433)
    %1 = ttg.memdesc_subview %arg2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc433)
    ttng.inval_barrier %1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc434)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc431)
    %c1_i32 = arith.constant 1 : i32 loc(#loc431)
    %2 = ttg.memdesc_subview %arg1[%c1_i32, %c0_i32_3] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc431)
    ttng.inval_barrier %2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc432)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc433)
    %c1_i32_5 = arith.constant 1 : i32 loc(#loc433)
    %3 = ttg.memdesc_subview %arg2[%c1_i32_5, %c0_i32_4] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc433)
    ttng.inval_barrier %3 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc434)
    tt.return loc(#loc435)
  } loc(#loc429)
  tt.func private @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp8e5S8_128_64SLNVMMA_64_8_False_False_NVMMALAS[8, 128, 64]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, MDi64S8_1SLSSS_1_1_1_0_1_1_0_SSSLAS[8, 1]ASMD, constexpr[constexpr[8]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg1: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg2: !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0)) attributes {noinline = false} {
    ttg.local_dealloc %arg0 : !ttg.memdesc<8x128x64xf8E5M2, #shared, #smem, mutable> loc(#loc430)
    %c0_i32 = arith.constant 0 : i32 loc(#loc431)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc431)
    %0 = ttg.memdesc_subview %arg1[%c0_i32_0, %c0_i32] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc433)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc433)
    %1 = ttg.memdesc_subview %arg2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc431)
    %c1_i32 = arith.constant 1 : i32 loc(#loc431)
    %2 = ttg.memdesc_subview %arg1[%c1_i32, %c0_i32_3] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc433)
    %c1_i32_5 = arith.constant 1 : i32 loc(#loc433)
    %3 = ttg.memdesc_subview %arg2[%c1_i32_5, %c0_i32_4] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %3 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_6 = arith.constant 0 : i32 loc(#loc431)
    %c2_i32 = arith.constant 2 : i32 loc(#loc431)
    %4 = ttg.memdesc_subview %arg1[%c2_i32, %c0_i32_6] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %4 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_7 = arith.constant 0 : i32 loc(#loc433)
    %c2_i32_8 = arith.constant 2 : i32 loc(#loc433)
    %5 = ttg.memdesc_subview %arg2[%c2_i32_8, %c0_i32_7] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %5 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_9 = arith.constant 0 : i32 loc(#loc431)
    %c3_i32 = arith.constant 3 : i32 loc(#loc431)
    %6 = ttg.memdesc_subview %arg1[%c3_i32, %c0_i32_9] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %6 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_10 = arith.constant 0 : i32 loc(#loc433)
    %c3_i32_11 = arith.constant 3 : i32 loc(#loc433)
    %7 = ttg.memdesc_subview %arg2[%c3_i32_11, %c0_i32_10] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %7 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_12 = arith.constant 0 : i32 loc(#loc431)
    %c4_i32 = arith.constant 4 : i32 loc(#loc431)
    %8 = ttg.memdesc_subview %arg1[%c4_i32, %c0_i32_12] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %8 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_13 = arith.constant 0 : i32 loc(#loc433)
    %c4_i32_14 = arith.constant 4 : i32 loc(#loc433)
    %9 = ttg.memdesc_subview %arg2[%c4_i32_14, %c0_i32_13] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %9 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_15 = arith.constant 0 : i32 loc(#loc431)
    %c5_i32 = arith.constant 5 : i32 loc(#loc431)
    %10 = ttg.memdesc_subview %arg1[%c5_i32, %c0_i32_15] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %10 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_16 = arith.constant 0 : i32 loc(#loc433)
    %c5_i32_17 = arith.constant 5 : i32 loc(#loc433)
    %11 = ttg.memdesc_subview %arg2[%c5_i32_17, %c0_i32_16] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %11 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_18 = arith.constant 0 : i32 loc(#loc431)
    %c6_i32 = arith.constant 6 : i32 loc(#loc431)
    %12 = ttg.memdesc_subview %arg1[%c6_i32, %c0_i32_18] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %12 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_19 = arith.constant 0 : i32 loc(#loc433)
    %c6_i32_20 = arith.constant 6 : i32 loc(#loc433)
    %13 = ttg.memdesc_subview %arg2[%c6_i32_20, %c0_i32_19] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %13 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    %c0_i32_21 = arith.constant 0 : i32 loc(#loc431)
    %c7_i32 = arith.constant 7 : i32 loc(#loc431)
    %14 = ttg.memdesc_subview %arg1[%c7_i32, %c0_i32_21] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc431)
    ttng.inval_barrier %14 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc432)
    %c0_i32_22 = arith.constant 0 : i32 loc(#loc433)
    %c7_i32_23 = arith.constant 7 : i32 loc(#loc433)
    %15 = ttg.memdesc_subview %arg2[%c7_i32_23, %c0_i32_22] : !ttg.memdesc<8x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc433)
    ttng.inval_barrier %15 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 8x1> loc(#loc434)
    tt.return loc(#loc435)
  } loc(#loc429)
  tt.func private @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp32S2_128_64SLTL128xconstexpr[64]UTLLAS['2', '128', '64']ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, MDi64S2_1SLSSS_1_1_1_0_1_1_0_SSSLAS[2, 1]ASMD, constexpr[constexpr[2]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<2x128x64xf32, #tmem, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg1: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg2: !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0)) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc431)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc431)
    %0 = ttg.memdesc_subview %arg1[%c0_i32_0, %c0_i32] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc431)
    ttng.inval_barrier %0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc432)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc433)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc433)
    %1 = ttg.memdesc_subview %arg2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc433)
    ttng.inval_barrier %1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc434)
    %c0_i32_3 = arith.constant 0 : i32 loc(#loc431)
    %c1_i32 = arith.constant 1 : i32 loc(#loc431)
    %2 = ttg.memdesc_subview %arg1[%c1_i32, %c0_i32_3] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc431)
    ttng.inval_barrier %2 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc432)
    %c0_i32_4 = arith.constant 0 : i32 loc(#loc433)
    %c1_i32_5 = arith.constant 1 : i32 loc(#loc433)
    %3 = ttg.memdesc_subview %arg2[%c1_i32_5, %c0_i32_4] : !ttg.memdesc<2x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc433)
    ttng.inval_barrier %3 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 2x1> loc(#loc434)
    tt.return loc(#loc435)
  } loc(#loc429)
  tt.func private @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDfp32S1_128_128SLTL128xconstexpr[128]UTLLAS['1', '128', '128']ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<1x128x128xf32, #tmem1, #ttng.tensor_memory, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0)) attributes {noinline = false} {
    %c0_i32 = arith.constant 0 : i32 loc(#loc431)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc431)
    %0 = ttg.memdesc_subview %arg1[%c0_i32_0, %c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc431)
    ttng.inval_barrier %0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc432)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc433)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc433)
    %1 = ttg.memdesc_subview %arg2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc433)
    ttng.inval_barrier %1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc434)
    tt.return loc(#loc435)
  } loc(#loc429)
  tt.func private @"__main__.Channel.<locals>.ChannelType.release____main__.Channel.<locals>.ChannelType<MDi8S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, MDi64S1_1SLSSS_1_1_1_0_1_1_0_SSSLAS[1, 1]ASMD, constexpr[constexpr[1]], constexpr[constexpr[1]]>__"(%arg0: !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg1: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0), %arg2: !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":179:0)) attributes {noinline = false} {
    ttg.local_dealloc %arg0 : !ttg.memdesc<1x1xi8, #shared1, #smem, mutable> loc(#loc430)
    %c0_i32 = arith.constant 0 : i32 loc(#loc431)
    %c0_i32_0 = arith.constant 0 : i32 loc(#loc431)
    %0 = ttg.memdesc_subview %arg1[%c0_i32_0, %c0_i32] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc431)
    ttng.inval_barrier %0 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc432)
    %c0_i32_1 = arith.constant 0 : i32 loc(#loc433)
    %c0_i32_2 = arith.constant 0 : i32 loc(#loc433)
    %1 = ttg.memdesc_subview %arg2[%c0_i32_2, %c0_i32_1] : !ttg.memdesc<1x1xi64, #shared1, #smem, mutable> -> !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc433)
    ttng.inval_barrier %1 : !ttg.memdesc<1xi64, #shared1, #smem, mutable, 1x1> loc(#loc434)
    tt.return loc(#loc435)
  } loc(#loc429)
} loc(#loc)
#loc2 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":831:26)
#loc3 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":835:30)
#loc4 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":836:31)
#loc5 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":837:67)
#loc6 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":838:71)
#loc7 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":839:69)
#loc8 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":840:69)
#loc9 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":841:54)
#loc10 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":842:54)
#loc11 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":854:4)
#loc12 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":855:4)
#loc13 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":856:4)
#loc14 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":857:4)
#loc15 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":858:4)
#loc16 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":859:4)
#loc17 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":860:4)
#loc18 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":861:4)
#loc20 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":233:77)
#loc21 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":233:11)
#loc22 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":233:4)
#loc23 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":131:0)
#loc24 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":133:49)
#loc25 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":134:71)
#loc26 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":135:71)
#loc27 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":137:39)
#loc28 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":137:22)
#loc29 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":138:39)
#loc30 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":138:22)
#loc31 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":139:41)
#loc32 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":139:24)
#loc33 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":140:11)
#loc34 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":140:4)
#loc36 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":788:32)
#loc37 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":789:32)
#loc38 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":790:21)
#loc39 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":791:21)
#loc40 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":792:17)
#loc41 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":794:19)
#loc42 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":796:40)
#loc43 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":797:63)
#loc44 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":798:37)
#loc45 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":799:44)
#loc46 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":800:32)
#loc47 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":800:39)
#loc48 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":802:39)
#loc49 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":803:24)
#loc50 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":804:39)
#loc51 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":805:24)
#loc52 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":807:41)
#loc53 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":807:23)
#loc54 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":808:103)
#loc55 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":809:103)
#loc56 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":809:12)
#loc57 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":812:68)
#loc58 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":814:68)
#loc59 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":813:8)
#loc60 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":797:4)
#loc62 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":176:39)
#loc63 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":176:56)
#loc64 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":176:11)
#loc65 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":176:4)
#loc67 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":172:39)
#loc68 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":172:56)
#loc69 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":172:11)
#loc70 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":172:4)
#loc72 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":370:30)
#loc73 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":371:38)
#loc74 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":372:27)
#loc75 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":373:35)
#loc76 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":374:28)
#loc77 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":375:11)
#loc78 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":375:4)
#loc80 = loc("/root/triton/python/triton/language/standard.py":40:16)
#loc81 = loc("/root/triton/python/triton/language/standard.py":40:22)
#loc82 = loc("/root/triton/python/triton/language/standard.py":40:28)
#loc83 = loc("/root/triton/python/triton/language/standard.py":40:11)
#loc84 = loc("/root/triton/python/triton/language/standard.py":40:4)
#loc86 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":379:26)
#loc87 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":380:29)
#loc88 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":381:40)
#loc89 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":381:53)
#loc90 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":382:37)
#loc91 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":382:27)
#loc92 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":383:23)
#loc93 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":383:49)
#loc94 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":384:42)
#loc95 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":384:11)
#loc96 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":384:4)
#loc98 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":344:22)
#loc99 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":345:21)
#loc100 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":347:37)
#loc101 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":347:24)
#loc102 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":347:55)
#loc103 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":347:47)
#loc104 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":348:39)
#loc105 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":348:29)
#loc106 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":350:11)
#loc107 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":350:4)
#loc109 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":406:15)
#loc110 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":405:4)
#loc112 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":216:63)
#loc113 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":217:64)
#loc114 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":218:11)
#loc115 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":218:4)
#loc117 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":163:25)
#loc118 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":164:38)
#loc119 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":165:38)
#loc120 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":167:29)
#loc121 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":168:11)
#loc122 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":168:4)
#loc124 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":145:28)
#loc125 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":145:40)
#loc126 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":145:15)
#loc127 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":146:25)
#loc128 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":147:29)
#loc129 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":148:34)
#loc130 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":149:39)
#loc131 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":149:42)
#loc132 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":150:11)
#loc133 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":150:4)
#loc135 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":724:33)
#loc136 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":725:39)
#loc137 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":725:52)
#loc138 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":726:20)
#loc139 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":727:44)
#loc140 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":727:63)
#loc141 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":729:32)
#loc142 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":731:49)
#loc143 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":732:23)
#loc144 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":733:32)
#loc145 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":733:26)
#loc146 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":734:20)
#loc147 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":735:20)
#loc148 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":736:11)
#loc149 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":736:4)
#loc151 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":484:51)
#loc152 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":486:68)
#loc153 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":486:11)
#loc154 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":486:4)
#loc156 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":464:8)
#loc157 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":453:11)
#loc158 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":453:4)
#loc160 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":743:33)
#loc161 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":744:56)
#loc162 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":745:24)
#loc163 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":745:56)
#loc164 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":746:33)
#loc165 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":747:24)
#loc166 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":747:56)
#loc167 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":748:33)
#loc168 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":749:20)
#loc169 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":751:36)
#loc170 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":752:32)
#loc171 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":763:16)
#loc172 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":765:42)
#loc173 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":766:23)
#loc174 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":767:32)
#loc175 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":767:26)
#loc176 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":768:34)
#loc177 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":768:61)
#loc178 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":768:56)
#loc179 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":770:4)
#loc180 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":771:20)
#loc181 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":772:20)
#loc182 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":774:19)
#loc183 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":774:11)
#loc184 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":776:28)
#loc185 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":777:64)
#loc186 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":777:14)
#loc187 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":778:31)
#loc188 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":778:17)
#loc189 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":778:46)
#loc190 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":779:44)
#loc191 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":779:21)
#loc192 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":781:11)
#loc193 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":781:4)
#loc195 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":491:53)
#loc196 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":492:53)
#loc197 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":494:70)
#loc198 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":495:70)
#loc199 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":496:11)
#loc200 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":496:4)
#loc202 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":199:63)
#loc203 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":200:64)
#loc204 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":201:11)
#loc205 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":201:4)
#loc207 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":154:25)
#loc208 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":155:38)
#loc209 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":156:38)
#loc210 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":158:29)
#loc211 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":159:11)
#loc212 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":159:4)
#loc214 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":688:56)
#loc215 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":688:4)
#loc217 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":648:42)
#loc218 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":650:17)
#loc219 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":651:20)
#loc220 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":652:33)
#loc221 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":654:40)
#loc222 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":655:63)
#loc223 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":656:37)
#loc224 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":658:32)
#loc225 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":659:86)
#loc226 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":659:18)
#loc227 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":661:67)
#loc228 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":662:57)
#loc229 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":667:37)
#loc230 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":673:36)
#loc231 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":674:60)
#loc232 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":675:57)
#loc233 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":675:61)
#loc234 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":675:23)
#loc235 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":676:57)
#loc236 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":676:61)
#loc237 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":676:23)
#loc238 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":678:24)
#loc239 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":679:37)
#loc240 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":681:24)
#loc241 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":681:8)
#loc242 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":655:4)
#loc244 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":601:34)
#loc245 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":603:33)
#loc246 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":604:36)
#loc247 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":605:25)
#loc248 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":613:42)
#loc249 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":613:47)
#loc250 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":613:31)
#loc251 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":614:30)
#loc252 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":614:24)
#loc253 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":616:48)
#loc254 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":617:61)
#loc255 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":617:65)
#loc256 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":617:25)
#loc257 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":618:24)
#loc258 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":620:45)
#loc259 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":620:28)
#loc260 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":621:34)
#loc261 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":621:29)
#loc262 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":621:28)
#loc263 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":622:31)
#loc264 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":622:87)
#loc265 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":622:20)
#loc266 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":624:40)
#loc267 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":625:21)
#loc268 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":626:24)
#loc269 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":626:57)
#loc270 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":626:51)
#loc271 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":627:21)
#loc272 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":628:42)
#loc273 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":628:75)
#loc274 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":628:69)
#loc275 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":629:24)
#loc276 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":631:37)
#loc277 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":633:24)
#loc278 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":633:42)
#loc279 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":633:53)
#loc280 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":634:33)
#loc281 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":635:22)
#loc282 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":636:20)
#loc283 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":636:28)
#loc284 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":637:8)
#loc285 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":639:11)
#loc286 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":639:4)
#loc288 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":422:17)
#loc289 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":423:11)
#loc290 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":423:4)
#loc292 = loc("/root/triton/python/triton/language/standard.py":188:40)
#loc293 = loc("/root/triton/python/triton/language/standard.py":188:15)
#loc294 = loc("/root/triton/python/triton/language/standard.py":176:4)
#loc296 = loc("/root/triton/python/triton/language/standard.py":167:27)
#loc297 = loc("/root/triton/python/triton/language/standard.py":167:11)
#loc298 = loc("/root/triton/python/triton/language/standard.py":167:4)
#loc300 = loc("/root/triton/python/triton/experimental/gluon/language/_standard.py":69:8)
#loc301 = loc("/root/triton/python/triton/experimental/gluon/language/_standard.py":65:11)
#loc302 = loc("/root/triton/python/triton/experimental/gluon/language/_standard.py":65:4)
#loc304 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":444:8)
#loc305 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":433:11)
#loc306 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":433:4)
#loc308 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":478:29)
#loc309 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":479:62)
#loc310 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":479:11)
#loc311 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":479:4)
#loc313 = loc("/root/triton/python/triton/language/standard.py":290:36)
#loc314 = loc("/root/triton/python/triton/language/standard.py":290:11)
#loc315 = loc("/root/triton/python/triton/language/standard.py":290:4)
#loc317 = loc("/root/triton/python/triton/language/standard.py":260:15)
#loc318 = loc("/root/triton/python/triton/language/standard.py":260:11)
#loc319 = loc("/root/triton/python/triton/language/standard.py":260:4)
#loc321 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":695:56)
#loc322 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":695:4)
#loc324 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":540:17)
#loc325 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":541:18)
#loc326 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":542:17)
#loc327 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":544:18)
#loc328 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":545:18)
#loc329 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":547:40)
#loc330 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":548:63)
#loc331 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":549:37)
#loc332 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":550:44)
#loc333 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":551:25)
#loc334 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":551:32)
#loc335 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":553:38)
#loc336 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":554:37)
#loc337 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":555:39)
#loc338 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":556:44)
#loc339 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":556:53)
#loc340 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":558:38)
#loc341 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":559:39)
#loc342 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":560:44)
#loc343 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":560:53)
#loc344 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":562:37)
#loc345 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":563:38)
#loc346 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":564:39)
#loc347 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":565:41)
#loc348 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":566:37)
#loc349 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":567:17)
#loc350 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":569:34)
#loc351 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":569:23)
#loc352 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":570:41)
#loc353 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":571:48)
#loc354 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":571:57)
#loc355 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":573:42)
#loc356 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":574:43)
#loc357 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":575:45)
#loc358 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":576:41)
#loc359 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":577:21)
#loc360 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":579:48)
#loc361 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":579:57)
#loc362 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":581:41)
#loc363 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":582:42)
#loc364 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":583:43)
#loc365 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":584:45)
#loc366 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":585:41)
#loc367 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":586:21)
#loc368 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":586:12)
#loc369 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":588:23)
#loc370 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":589:23)
#loc371 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":591:38)
#loc372 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":592:39)
#loc373 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":593:41)
#loc374 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":594:37)
#loc375 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":594:8)
#loc376 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":548:4)
#loc378 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":253:28)
#loc379 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":253:4)
#loc381 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":504:17)
#loc382 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":505:18)
#loc383 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":507:40)
#loc384 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":508:63)
#loc385 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":509:37)
#loc386 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":510:44)
#loc387 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":512:39)
#loc388 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":513:38)
#loc389 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":514:54)
#loc390 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":516:37)
#loc391 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":517:37)
#loc392 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":518:52)
#loc393 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":520:39)
#loc394 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":521:38)
#loc395 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":522:54)
#loc396 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":524:37)
#loc397 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":525:52)
#loc398 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":527:34)
#loc399 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":527:54)
#loc400 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":528:41)
#loc401 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":529:41)
#loc402 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":530:56)
#loc403 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":531:41)
#loc404 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":532:56)
#loc405 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":532:12)
#loc406 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":527:8)
#loc407 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":508:4)
#loc409 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":247:25)
#loc410 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":248:60)
#loc411 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":248:4)
#loc413 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":703:19)
#loc414 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":704:40)
#loc415 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":705:63)
#loc416 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":706:37)
#loc417 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":708:40)
#loc418 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":709:68)
#loc419 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":709:92)
#loc420 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":711:40)
#loc421 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":712:68)
#loc422 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":712:92)
#loc423 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":714:23)
#loc424 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":715:24)
#loc425 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":716:23)
#loc426 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":717:24)
#loc427 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":717:8)
#loc428 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":705:4)
#loc430 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":181:8)
#loc431 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":183:50)
#loc432 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":183:28)
#loc433 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":184:50)
#loc434 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":184:28)
#loc435 = loc("/root/triton/python/tutorials/gluon/01-attention-forward.py":182:4)
